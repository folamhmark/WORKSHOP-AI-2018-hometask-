{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lection_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/folamhmark/WORKSHOP-AI-2019-hometask-/blob/master/Lection_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nBq-At7dwmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rACJmcjnd8pi",
        "colab_type": "text"
      },
      "source": [
        "#Урок 4\n",
        "\n",
        "В данном ДЗ две части: по оптимизации и по регуляризации. В первой части необходимо реализовать алгоритмы, описанные на уроке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5MGPYypeSaQ",
        "colab_type": "text"
      },
      "source": [
        "### Momentum GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmOoGIHGeOnY",
        "colab_type": "code",
        "outputId": "dbd79d5e-8efd-4954-a470-93deb2a559df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "#Задача: инициализировать значение v для momentum GD\n",
        "def initialize_velocity(parameters):\n",
        "    \"\"\"\n",
        "    Initializes the velocity as a python dictionary with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    \n",
        "    Returns:\n",
        "    v -- python dictionary containing the current velocity.\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // # number of layers in the neural networks\n",
        "    v = {}\n",
        "    betta = 0.9\n",
        "    \n",
        "    # Initialize velocity\n",
        "    for l in range(L):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = 0\n",
        "        v[\"db\" + str(l+1)] = 0\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9e4576c0c2d2>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    L = len(parameters) // # number of layers in the neural networks\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0IX-rCMeX9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using Momentum\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- python dictionary containing the current velocity:\n",
        "                    v['dW' + str(l)] = ...\n",
        "                    v['db' + str(l)] = ...\n",
        "    beta -- the momentum hyperparameter, scalar\n",
        "    learning_rate -- the learning rate, scalar\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- python dictionary containing your updated velocities\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // # number of layers in the neural networks\n",
        "    \n",
        "    # Momentum update for each parameter\n",
        "    for l in range(L):\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # посчитайте значения v\n",
        "        v[\"dW\" + str(l+1)] = betta*v[\"dW\" + str(l+1)]+(1-betta)*grads['dW' + str(l)]\n",
        "        v[\"db\" + str(l+1)] = betta*v[\"db\" + str(l+1)]+(1-betta)*grads['db' + str(l)] \n",
        "        # обновите параметры\n",
        "        parameters[\"W\" + str(l+1)] = parameters['W' + str(l)] - learning_rate*v[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters['b' + str(l)] - learning_rate*v[\"db\" + str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return parameters, v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Ufn3ydfMrV",
        "colab_type": "text"
      },
      "source": [
        "### AdaM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zGWrqNd_Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Задание: инициализировать параметры v и s для алгоритма AdaM\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    betta1 = 0.9\n",
        "    betta2 = 0,999\n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "    ### START CODE HERE ### \n",
        "        v[\"dW\" + str(l+1)] = 0\n",
        "        v[\"db\" + str(l+1)] = 0\n",
        "        s[\"dW\" + str(l+1)] = 0\n",
        "        s[\"db\" + str(l+1)] = 0\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu2Lr4ETfUcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Задача: реализовать алгоритм AdaM\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) //                   # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Цикл для обновления параметров\n",
        "    for l in range(L):\n",
        "        # Подсчет скользящих средних градиентов. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        ### START CODE HERE ### \n",
        "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)grads[\"dW\" + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)grads[\"db\" + str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Подсчет v с учетом коррекции смещения (bias correction). Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        ### START CODE HERE ###\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-(beta1**t))\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-(beta1**t))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Подсчет скользящих средних для \"квадратов\" градиентов. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        ### START CODE HERE ###\n",
        "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*(grads[\"dW\" + str(l+1)]** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*(grads[\"db\" + str(l+1)]** 2)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Подсчет S с учетом коррекции смещения. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        ### START CODE HERE ###\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-(beta2**t))\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-(beta2**t))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Обновление параметров. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        ### START CODE HERE ###\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate*v_corrected[\"dW\" + str(l+1)])/((s_corrected[\"dW\" + str(l+1)] + E)**0.5)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate*v_corrected[\"db\" + str(l+1)])/((s_corrected[\"db\" + str(l+1)] + E)**0.5)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UVHUrTPfY_8",
        "colab_type": "text"
      },
      "source": [
        "## Регуляризация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY12hOa9fbCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}