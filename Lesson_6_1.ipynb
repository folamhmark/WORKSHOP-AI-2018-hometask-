{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson_6-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/folamhmark/WORKSHOP-AI-2019-hometask-/blob/master/Lesson_6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz4_lGlZ_A6G",
        "colab_type": "text"
      },
      "source": [
        "# Урок 6. Введение в библиотеки глубокого обучения: TensorFlow, Keras\n",
        "\n",
        "На предыдущих уроках мы познакомились с базовыми понятиями того, что из себя представляют нейронные сети. Мы самостоятельно реализовали однослойные (shallow) и многослойные (deep) полносвязные нейронные сети (fully connected NN). Однако на практике построение нейронных сетей с нуля не производят. На сегодняшний день доступно множество библиотек на разных языках программирования, которые позволяют эффективно решать задачи по deep learning. Среди них:\n",
        " - Tensorflow (Google);\n",
        " - Theano (University of Montreal);\n",
        " - PyTorch (Facebook);\n",
        " - Caffe (UC Berkeley);\n",
        " - MXNet (Apache);\n",
        " - CNTK (Microsoft).\n",
        " \n",
        "Все эти библиотеки имеют поддержку CUDA, то есть можно проводить вычисления на видеокартах nvidia. Интерфейс этих библиотек может достаточно сильно отличаться друг от друга по синтаксису, однако суть везде та же самая: библиотеки реализовывают шаг forward и backward propagation и осуществляют процесс обучения с заданной функцией ошибки и заданными гиперпараметрами (в т.ч. оптимизатором).\n",
        "\n",
        "## Keras\n",
        "\n",
        "$\\textbf{Keras}$ тоже часто называют библиотекой глубокого обучения, однако это неверно. Keras - это библиотека и интерфейс, упрощающий взаимодействие между пользователем и непосредственно библиотекой глубокого обучения (Keras поддерживает TensorFlow, Theano и CNTK). Его интерфейс максимально простой и интуитивно понятный, вследствие чего он хорошо подходит для быстрого проектирования нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ_BQj5nDapS",
        "colab_type": "text"
      },
      "source": [
        "## Пример задач на TensorFlow\n",
        "\n",
        "Ниже приведена реализация функции ошибки MSE\n",
        "\n",
        "$$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2: \\tag{1}$$\n",
        "\n",
        "```python\n",
        "y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\n",
        "y = tf.constant(39, name='y')                    # Define y. Set to 39\n",
        "\n",
        "loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss\n",
        "\n",
        "init = tf.global_variables_initializer()         # When init is run later (session.run(init)),\n",
        "                                                 # the loss variable will be initialized and ready to be computed\n",
        "with tf.Session() as session:                    # Create a session and print the output\n",
        "    session.run(init)                            # Initializes the variables\n",
        "    print(session.run(loss))                     # Prints the loss\n",
        "\n",
        "```\n",
        "\n",
        "Логика TensorFlow в следующем: сначала мы описываем последовательность вычислений, но при этом никакого подсчета не происходит. Далее мы должны начать т.н. \"сессию\" (session): во время сессии будут последовательно выполняться описанные нами ранее вычисления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVPPYvItJ0At",
        "colab_type": "text"
      },
      "source": [
        "### Пример 1. Минимизация функции $y = (w-5)^2$ по $w$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFahnvEgDMGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_2D-FFiD-wB",
        "colab_type": "code",
        "outputId": "b1bac505-3e97-4594-ad91-f55fc658fe2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = tf.Variable(0,dtype = tf.float32)\n",
        "cost = tf.add(tf.add(w**2, tf.multiply(-10.,w)), 25)\n",
        "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "session = tf.Session()\n",
        "session.run(init)\n",
        "print(session.run(w))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Wjh_hADQP1",
        "colab_type": "code",
        "outputId": "4ba85a55-1d54-4ab8-f4d6-cdd7a9e445bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "session.run(train)\n",
        "print(session.run(w))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.099999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyzSMi8JJRbe",
        "colab_type": "code",
        "outputId": "74ed628a-ecdf-457a-e67d-6ea3107f6e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(100):\n",
        "  session.run(train)\n",
        "  print(session.run(w))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.198\n",
            "0.29404\n",
            "0.3881592\n",
            "0.480396\n",
            "0.5707881\n",
            "0.6593723\n",
            "0.7461849\n",
            "0.83126116\n",
            "0.91463596\n",
            "0.99634326\n",
            "1.0764164\n",
            "1.154888\n",
            "1.2317903\n",
            "1.3071545\n",
            "1.3810115\n",
            "1.4533913\n",
            "1.5243235\n",
            "1.593837\n",
            "1.6619602\n",
            "1.728721\n",
            "1.7941467\n",
            "1.8582637\n",
            "1.9210985\n",
            "1.9826765\n",
            "2.0430229\n",
            "2.1021624\n",
            "2.160119\n",
            "2.2169166\n",
            "2.2725782\n",
            "2.3271267\n",
            "2.3805842\n",
            "2.4329727\n",
            "2.4843132\n",
            "2.534627\n",
            "2.5839343\n",
            "2.6322556\n",
            "2.6796105\n",
            "2.7260182\n",
            "2.7714977\n",
            "2.8160677\n",
            "2.8597465\n",
            "2.9025514\n",
            "2.9445004\n",
            "2.9856105\n",
            "3.0258982\n",
            "3.0653803\n",
            "3.1040728\n",
            "3.1419914\n",
            "3.1791515\n",
            "3.2155685\n",
            "3.2512572\n",
            "3.286232\n",
            "3.3205073\n",
            "3.3540971\n",
            "3.387015\n",
            "3.4192748\n",
            "3.4508893\n",
            "3.4818716\n",
            "3.5122342\n",
            "3.5419896\n",
            "3.5711498\n",
            "3.599727\n",
            "3.6277323\n",
            "3.6551776\n",
            "3.682074\n",
            "3.7084327\n",
            "3.7342641\n",
            "3.759579\n",
            "3.7843874\n",
            "3.8086996\n",
            "3.8325257\n",
            "3.8558753\n",
            "3.8787577\n",
            "3.9011827\n",
            "3.923159\n",
            "3.9446957\n",
            "3.9658017\n",
            "3.9864857\n",
            "4.006756\n",
            "4.026621\n",
            "4.046088\n",
            "4.0651665\n",
            "4.0838633\n",
            "4.102186\n",
            "4.1201425\n",
            "4.1377397\n",
            "4.154985\n",
            "4.171885\n",
            "4.1884475\n",
            "4.2046785\n",
            "4.220585\n",
            "4.236173\n",
            "4.2514496\n",
            "4.2664204\n",
            "4.281092\n",
            "4.29547\n",
            "4.309561\n",
            "4.3233695\n",
            "4.336902\n",
            "4.350164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrGJjXo3KjxN",
        "colab_type": "text"
      },
      "source": [
        "**Замечание 1.** На примере видно, что теперь нам можно не заботиться о процедуре взятия производных - это выполняет функция minimize класса GradientDescentOptimizer. На практике это означает, что нам достаточно реализовать шаг forward propagation, а backpropagation будет выполнен автоматически.\n",
        "\n",
        "**Замечание 2.** Данная функция зависела только от одной переменной, значение которой было заранее известно. Однако функция ошибки в реальных задач глубокого обучения зависит еще и от входных данных, которые заранее неизвестны (на этапе проектирования нейросети). Чтобы обеспечить возможность работы с такими данными (грубо говоря, с входным массивом X), в TensorFlow существует специальный тип данных\n",
        "\n",
        "```python\n",
        "tf.placeholder(dtype, shape),\n",
        "```\n",
        "где dtype - это тип данных (int, float16, float32), а shape - это размерность входного массива."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huWIxXXlN0nf",
        "colab_type": "text"
      },
      "source": [
        "### Пример 2. Минимизация функции  $𝑦=x_0\\cdot w^2 - x_1 \\cdot w + x_2$  по  𝑤\n",
        "\n",
        "Пусть теперь функция ошибки определяется не только параметром $w$, но и даннымы $x$. Рассмотрим, как реализовать этот пример, используя тип данных tf.placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoq01epMNzw2",
        "colab_type": "code",
        "outputId": "29d1f2fb-57c1-41c7-df93-9c55f0c2f406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "coefficients = np.array([[1.,10.,25.]])\n",
        "\n",
        "w = tf.Variable(0,dtype = tf.float32)\n",
        "x = tf.placeholder(dtype = tf.float32, shape = [1,3])\n",
        "cost = x[0][0] * w**2 - x[0][1] * w + x[0][2]\n",
        "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "session = tf.Session()\n",
        "session.run(init)\n",
        "print(session.run(w))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewH3PsHiJnFX",
        "colab_type": "code",
        "outputId": "f0a6e49c-1845-4340-d901-9fa5f6c3a996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(1000):\n",
        "  session.run(train, feed_dict = {x:coefficients})\n",
        "print(session.run(w))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.999988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ASaBVlSGIP",
        "colab_type": "text"
      },
      "source": [
        "Написание программ на TensorFlow можно описать несколькими шагами:\n",
        "\n",
        "1. Объявить переменные Tensors (variables), над которыми никаких операций совершено пока не будет. \n",
        "2. Определить математические операции между этими Tensors.\n",
        "3. ИнициализироватьTensors. \n",
        "4. Создать сессию. \n",
        "5. Запустить сессию, в процессе которой будут непосредственно выполняться вышеописанные вычисления над переменными Tensors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XST9RIFiVyRg",
        "colab_type": "text"
      },
      "source": [
        "### Пример 3. Что будет, если не запустить Session?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5SNcwtaP9vK",
        "colab_type": "code",
        "outputId": "102c7e5c-bca3-45fe-edd8-4f2c5b8c99c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = tf.constant(2)\n",
        "b = tf.constant(10)\n",
        "c = tf.multiply(a,b)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Mul_6:0\", shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnvW73jdV8bk",
        "colab_type": "code",
        "outputId": "f641bb1e-a51d-4ed3-89a2-dadfacabc676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "print(sess.run(c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lru9Ir1IwTtV",
        "colab_type": "text"
      },
      "source": [
        "### Пример 4.  One-hot Encoding. Многоклассовая классификация и функция softmax\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1J0WflUO8DxWyQuHxcOQYBE1GhfEyOdeE\" style=\"width:665px;height:320px;\">\n",
        "\n",
        "Пусть перед нами поставлена задача многоклассовой классификации. Например, нам надо предсказать цвет светофора по каким-то входным данным. Тогда классов будет три: [красный, желтый, зеленый]. Мы уже не можем воспользоваться единственным числом на выходе нейросети, чтобы получить ответ. Необходимо, чтобы нейросеть выдавала выход размерности 3 (3 в данной задаче). Тогда необходимо каждый цвет светофора тоже закодировать вектором. Простейший способ это сделать - one-hot encoding. Тогда:\n",
        " - красный = $(1, 0, 0)^T$;\n",
        " - желтый = $(0,1,0)^T$;\n",
        " - зеленый = $(0,0,1)^T$.\n",
        " \n",
        "На выходе нейросети будет трехмерный вектор, i-ую координату которого можно интерпретировать как вероятность принадлежности объекта к i-ому классу. \n",
        "Используется следующая функция потерь:\n",
        "\n",
        "$$J(y, \\hat y) = -\\frac{1}{N} \\sum_{s\\in S} \\sum_{c \\in C} 1_{s\\in c} \\log {p(s \\in c)}.$$\n",
        "\n",
        "Сумма всех координат должна равняться единице:\n",
        "\n",
        "$$\\forall l \\in[1,N]  \\quad\\sum_{i=1}^m y_i^{(l)} = 1,$$\n",
        "\n",
        "где N -кол-во объектов, l -кол-во классов. Чтобы такое требование выполнялось, вводится новая функция активации, которая называется $\\textit{Softmax}:$\n",
        "\n",
        "$$Softmax(z)_i = \\frac{\\exp{(z_i)}}{\\sum_{j=1}^m \\exp{(z_j)}}, \\quad i \\in [1,m]. $$\n",
        "\n",
        "Таким образом, требования, чтобы сумма всех координат равнялась единице, автоматически выполняется (равно как и то, что вероятность - это неотрицательная величина).\n",
        "\n",
        "В TensorFlow есть инструмент, которые позволяет преобразовать данные y в one-hot векторы:\n",
        "\n",
        "- tf.one_hot(labels, depth, axis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSHj-iPo2dh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_matrix(labels, C):\n",
        "    \"\"\"\n",
        "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
        "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
        "                     will be 1. \n",
        "                     \n",
        "    Arguments:\n",
        "    labels -- vector containing the labels \n",
        "    C -- number of classes, the depth of the one hot dimension\n",
        "    \n",
        "    Returns: \n",
        "    one_hot -- one hot matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create a tf.constant equal to C (depth), name it 'C'.\n",
        "    C = tf.constant(C, name = 'C')\n",
        "    \n",
        "    # Use tf.one_hot, be careful with the axis\n",
        "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
        "    \n",
        "    # Create the session\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    # Run the session\n",
        "    one_hot = sess.run(one_hot_matrix)\n",
        "    \n",
        "    # Close the session\n",
        "    sess.close()\n",
        "    \n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS_f40y22eT8",
        "colab_type": "code",
        "outputId": "4b3db42a-a688-4db0-fbb3-b5811214b4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "labels = np.array([1,2,3,0,2,1])\n",
        "one_hot = one_hot_matrix(labels, C = 4)\n",
        "print (\"one_hot = \" + str(one_hot))\n",
        "print(type(one_hot))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one_hot = [[0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT9WDx1i6uCn",
        "colab_type": "text"
      },
      "source": [
        "### Пример 5. tf.layers\n",
        "\n",
        "В TensorFlow можно либо прописывать самому цепочку вычислений, происходящих на каждом слое, либо воспользоваться готовой реализацией из раздела tf.layers. Мы пока знакомы только с полносвязными нейросетями (fully connected NN), их слои называются Dense layers. Пример внизу показывает, как работает такой слой (без шага обучения!):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Wck8-n7CbU",
        "colab_type": "code",
        "outputId": "ece0f61f-3b33-4202-b66e-20c771b60de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "y = tf.layers.dense(x, units=1, kernel_initializer=tf.zeros_initializer())\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "print(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0722 09:57:01.969895 140552640759680 deprecation.py:323] From <ipython-input-21-7d9f41f4ec6c>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm82Jk8Y7tRE",
        "colab_type": "text"
      },
      "source": [
        "### Пример 6. Линейная регрессия\n",
        "\n",
        "Линейная регрессия - это задача машинного обучения, заключающаяся в линейной аппроксимации целевой переменной y:\n",
        "\n",
        "$$ y = Wx + b, \\quad J = \\sum(\\hat y_i - y_i)^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbkNFbY7uHp",
        "colab_type": "code",
        "outputId": "43e749d5-658f-4f3e-f507-a74660e61121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
        "y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\n",
        "\n",
        "linear_model = tf.layers.Dense(units=1)\n",
        "\n",
        "y_pred = linear_model(x)\n",
        "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "for i in range(1000):\n",
        "  _, loss_value = sess.run((train, loss))\n",
        "  #print(loss_value)\n",
        "\n",
        "print(sess.run(y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.04127139]\n",
            " [-1.0199988 ]\n",
            " [-1.9987264 ]\n",
            " [-2.9774537 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXPnEJ2SWr5v",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1. Реализация линейной функции  $y = W \\cdot x + b$\n",
        "\n",
        "Первым заданием будет подсчет следующего выражения: $Y = WX + b$, где $W$и  $X$ - это случайные матрицы, а b - случайный вектор. \n",
        "\n",
        "**Задание**: Рассчитать $W\\cdot x + b$, где $W, x $, и $b$ получены из стандартного нормального распределения. W имеет размер (4, 3), x - (3,1), b - (4,1). В кач-ве примера показано, как реализовать объявление X с размером (3,1):\n",
        "```python\n",
        "X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
        "\n",
        "```\n",
        "Также обратите внимание на следующие функции: \n",
        "- tf.matmul(..., ...) - перемножение матриц;\n",
        "- tf.add(..., ...) - сложение двух объектов;\n",
        "- np.random.randn(...) - метод numpy для инициализации массивов случайными значениями.\n",
        "\n",
        "**NB** Обратите внимание на аргумент name функции tf.constant. Не забудьте его тоже правильно указать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv4pXof4XKGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_function():\n",
        "    \"\"\"\n",
        "    Implements a linear function: \n",
        "            Initializes W to be a random tensor of shape (4,3)\n",
        "            Initializes X to be a random tensor of shape (3,1)\n",
        "            Initializes b to be a random tensor of shape (4,1)\n",
        "    Returns: \n",
        "    result -- runs the session for Y = WX + b \n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    ### START CODE HERE ### (4 lines of code)\n",
        "    X = tf.constant(np.random.randn(3, 1), name=\"X\")\n",
        "    W = tf.constant(np.random.randn(4, 3), name=\"W\")\n",
        "    b = tf.constant(np.random.randn(4, 1), name=\"b\")\n",
        "    Y = tf.add(tf.matmul(W, X), b)\n",
        "    ### END CODE HERE ### \n",
        "    \n",
        "    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    sess = tf.Session()\n",
        "    result = sess.run(Y)\n",
        "    ### END CODE HERE ### \n",
        "    \n",
        "    # close the session \n",
        "    sess.close()\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux5tNrrOZt6w",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2. Реализация функции активации (сигмоиды)\n",
        "\n",
        "Итак, мы реализовали сверху линейную часть шага forward_propagation. Tensorflow уже имеет реализации таких функций активации как сигмоида или ReLU. В данном задании необходимо будет реализовать шаг подсчета сигмоиды. \n",
        "\n",
        "Для выполнения этого задания надо будет воспользоваться типом tf.placeholder. При запуске сессии надо будет передать словарь feed_dict со значением z (результат предыдущей функции).\n",
        "Порядок будет следующим:\n",
        " - создать placeholder x;\n",
        " - записать вычисления, которые над ним будут проводиться (tf.sigmoid);\n",
        " - запустить сессию.\n",
        "\n",
        "Сессию можно запустить двумя способами: \n",
        "\n",
        "**Способ 1:**\n",
        "```python\n",
        "sess = tf.Session()\n",
        "# Вычислительные шаги\n",
        "result = sess.run(..., feed_dict = {...})\n",
        "sess.close() # Закрытие сессии\n",
        "```\n",
        "** Способ 2:**\n",
        "```python\n",
        "with tf.Session() as sess: \n",
        "    # Вычислительные шаги\n",
        "    result = sess.run(..., feed_dict = {...})\n",
        "    # Нет необходимости явно закрывать сессию :)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo89bUribRt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid of z\n",
        "    \n",
        "    Arguments:\n",
        "    z -- input value, scalar or vector\n",
        "    \n",
        "    Returns: \n",
        "    results -- the sigmoid of z\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Create a placeholder for x. Name it 'x'.\n",
        "    x = tf.placeholder(tf.float32, name=\"x\")\n",
        "\n",
        "    # compute sigmoid(x)\n",
        "    \n",
        "    sigmoid = tf.sigmoid(x)\n",
        "\n",
        "    # Create a session, and run it.\n",
        "    # You should use a feed_dict to pass z's value to x. \n",
        "    \n",
        "    # Run session and call the output \"result\"\n",
        "    with tf.Session() as sess:\n",
        "        # Run session and call the output \"result\"\n",
        "        result = sess.run(sigmoid, feed_dict={x: z})\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXS-R_dGcKn7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Промежуточное summary** \n",
        "\n",
        "На данном этапе мы узнали несколько ключевых вещей...\n",
        "  \n",
        "1. Мы узнали, что такое tf.Variable и что такое tf.placeholder.\n",
        "2. Научились объявлять вычислительные операции.\n",
        "3. Научились создавать сессию.\n",
        "4. Научились осуществлять сессию с использованием feed_dict - словаря для входных данных. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lMAiAzndK9U",
        "colab_type": "text"
      },
      "source": [
        "### Задание 3 -  Подсчет функции ошибки\n",
        "\n",
        "Многие функции ошибки также содержатся в библиотеке TensorFlow, в том числе бинарная кросс-энтропия: \n",
        "$$ J = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log a^{ [2] (i)} + (1-y^{(i)})\\log (1-a^{ [2] (i)} )\\large ).\\small\\tag{2}$$\n",
        "\n",
        "\n",
        "**Задание**: Реализуйте подсчет функции ошибки, используя следующую функцию: \n",
        "\n",
        "\n",
        "- `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\n",
        "\n",
        "Шаги должны быть следующими: подать сигмоиде входные данные z, посчитать сигмоиду, затем вызвать функцию ошибки, где logits - это вывод нейросети до применения сигмоиды, labels - истинный ответ. Таким образом, будет осуществляться подсчет следующего выражения:\n",
        "\n",
        "$$- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwmsy19zb7PQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(logits, labels):\n",
        "    \"\"\"\n",
        "    Computes the cost using the sigmoid cross entropy\n",
        "    \n",
        "    Arguments:\n",
        "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
        "    labels -- vector of labels y (1 or 0) \n",
        "    \n",
        "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
        "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
        "    \n",
        "    Returns:\n",
        "    cost -- runs the session of the cost (formula (2))\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### \n",
        "    \n",
        "    # Create the placeholders for \"logits\" (z) and \"labels\" (y)\n",
        "    z = tf.placeholder(tf.float32, name=\"z\")\n",
        "    y = tf.placeholder(tf.float32, name=\"y\")\n",
        "    \n",
        "    \n",
        "    # Use the loss function \n",
        "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
        "    \n",
        "    # Create a session (approx. 1 line). See method 1 above.\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    # Run the session (approx. 1 line).\n",
        "    cost = sess.run(cost, feed_dict={z: logits, y: labels})\n",
        "    \n",
        "    # Close the session (approx. 1 line). See method 1 above.\n",
        "    sess.close()\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n42pi1afqbzh",
        "colab_type": "text"
      },
      "source": [
        "##Keras\n",
        "\n",
        "Как уже было сказано ранее, Keras - это высокоуровневая оболочка над фреймворком для глубокого обучения (по умолчанию Keras использует ядро TensorFlow). Далее мы рассмотрим несколько примеров проектирования нейросетей на Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EVC-cxFq1I0",
        "colab_type": "text"
      },
      "source": [
        "### Пример 1. Двуслойная нейросеть для многоклассовой классификации\n",
        "\n",
        "Используется нейросеть со следующими параметрами:\n",
        " - вход имеет размерность (1,100);\n",
        " - первый слой имеет 32 нейрона, функция активации ReLU;\n",
        " - второй слой имеет 10 нейронов, функция активации Softmax;\n",
        " - в качестве оптимизатора используется RMSProp;\n",
        " - регуляризаторы не используются;\n",
        " - функция ошибки: категориальная кросс-энтропия:\n",
        " $$ \\large CE = - \\large \\sum_i^C y_i \\log(Softmax(z)_i) = - \\large \\log(\\frac{\\exp{(z_i)}}{\\sum_{j=1}^m \\exp{(z_j)}}),$$\n",
        " \n",
        " где C -  количество различных классов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUccb3rr5_fE",
        "colab_type": "code",
        "outputId": "6dc9b71e-9848-4782-9da9-a094cb5f5bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "import numpy as np\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=100))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data\n",
        "\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(10, size=(1000, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "W0722 10:10:02.283468 140552640759680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0722 10:10:02.285250 140552640759680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0722 10:10:02.292091 140552640759680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0722 10:10:02.324480 140552640759680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0722 10:10:02.346895 140552640759680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0722 10:10:02.538162 140552640759680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 0s 374us/step - loss: 2.3363 - acc: 0.0920\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 106us/step - loss: 2.3013 - acc: 0.1190\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 108us/step - loss: 2.2907 - acc: 0.1320\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 118us/step - loss: 2.2774 - acc: 0.1360\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 123us/step - loss: 2.2695 - acc: 0.1490\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 115us/step - loss: 2.2614 - acc: 0.1330\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 107us/step - loss: 2.2491 - acc: 0.1470\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 127us/step - loss: 2.2424 - acc: 0.1600\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 128us/step - loss: 2.2321 - acc: 0.1720\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 128us/step - loss: 2.2250 - acc: 0.1820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4a00ee710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQCbCkN8s-et",
        "colab_type": "text"
      },
      "source": [
        "Шаги, проделанные в Keras для обучения нейросети, похожи на те, что мы проделывали в TensorFlow:\n",
        " \n",
        " - 1) описание вычислительной модели - объект класса Sequential;\n",
        " - 2) задание оптимизаторов и функции ошибки - метод compile;\n",
        " - 3) обучение модели - метод  fit.\n",
        "Тем не менее, интерфейс Keras более user-friendly, что наглядно видно на данном примере.\n",
        "\n",
        "**Важное дополнение** У класса model есть метод summary, который позволяет вывести всю информацию о модели: в каком слое сколько параметров и гиперпараметров, размерность каждого слоя и тд."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAYBe9YhuA35",
        "colab_type": "code",
        "outputId": "9c03b314-56ae-40fe-8cf3-bbf25d44fe4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 32)                3232      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 3,562\n",
            "Trainable params: 3,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBdS_IpuSo7",
        "colab_type": "text"
      },
      "source": [
        "В первом слое содержится 32x100 параметров $w_{ij}$ и  32 параметра $b_j$ - всего 3232 обучаемых параметра.\n",
        "\n",
        "Во втором слое содержится 10х32 параметра $w_{ij}$ и 10 параметров $b_j$ - всего 330 параметров.\n",
        "\n",
        "Данная архитектура не содержит т.н. non-trainable parameters (параметры, которые зависят от входных данных, но для которых не нужно подбирать оптимальные значения), они бы появились, если бы мы добавили batch_normalization. \n",
        "\n",
        "Тогда на каждом слое производился бы подсчет среднего и дисперсии - такие параметры как раз относятся к non-trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFSQszs8v8DK",
        "colab_type": "text"
      },
      "source": [
        "###Пример 2. Задача бинарной классификации на Keras\n",
        "\n",
        "Ниже приведен пример задачи, которую мы уже реализовывали с помощью средств numpy - это задача бинарной классификации (отнесение объекта либо к одному, либо к другому классу)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV6DZNLawPcw",
        "colab_type": "code",
        "outputId": "70730c1d-beb3-4b9f-b5c4-199a6ddcfd5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Generate dummy data\n",
        "x_train = np.random.random((1000, 20))\n",
        "y_train = np.random.randint(2, size=(1000, 1))\n",
        "x_test = np.random.random((100, 20))\n",
        "y_test = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=20, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128)\n",
        "score = model.evaluate(x_test, y_test, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0722 10:16:49.505267 140552640759680 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 0s 349us/step - loss: 0.7243 - acc: 0.4740\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 0s 39us/step - loss: 0.7171 - acc: 0.4760\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 0s 39us/step - loss: 0.6988 - acc: 0.5180\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 0s 39us/step - loss: 0.7082 - acc: 0.4950\n",
            "Epoch 5/20\n",
            "1000/1000 [==============================] - 0s 39us/step - loss: 0.7000 - acc: 0.5250\n",
            "Epoch 6/20\n",
            "1000/1000 [==============================] - 0s 37us/step - loss: 0.7026 - acc: 0.4960\n",
            "Epoch 7/20\n",
            "1000/1000 [==============================] - 0s 45us/step - loss: 0.7015 - acc: 0.4950\n",
            "Epoch 8/20\n",
            "1000/1000 [==============================] - 0s 38us/step - loss: 0.6982 - acc: 0.5210\n",
            "Epoch 9/20\n",
            "1000/1000 [==============================] - 0s 40us/step - loss: 0.6980 - acc: 0.5060\n",
            "Epoch 10/20\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 0.6983 - acc: 0.5130\n",
            "Epoch 11/20\n",
            "1000/1000 [==============================] - 0s 45us/step - loss: 0.6976 - acc: 0.5220\n",
            "Epoch 12/20\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 0.6971 - acc: 0.5080\n",
            "Epoch 13/20\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 0.6942 - acc: 0.5190\n",
            "Epoch 14/20\n",
            "1000/1000 [==============================] - 0s 40us/step - loss: 0.6968 - acc: 0.4990\n",
            "Epoch 15/20\n",
            "1000/1000 [==============================] - 0s 35us/step - loss: 0.6936 - acc: 0.5210\n",
            "Epoch 16/20\n",
            "1000/1000 [==============================] - 0s 36us/step - loss: 0.6923 - acc: 0.5230\n",
            "Epoch 17/20\n",
            "1000/1000 [==============================] - 0s 36us/step - loss: 0.6886 - acc: 0.5420\n",
            "Epoch 18/20\n",
            "1000/1000 [==============================] - 0s 38us/step - loss: 0.6915 - acc: 0.5260\n",
            "Epoch 19/20\n",
            "1000/1000 [==============================] - 0s 39us/step - loss: 0.6885 - acc: 0.5340\n",
            "Epoch 20/20\n",
            "1000/1000 [==============================] - 0s 37us/step - loss: 0.6882 - acc: 0.5380\n",
            "100/100 [==============================] - 0s 735us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es1NBQZjwgQe",
        "colab_type": "text"
      },
      "source": [
        "**ВАЖНО** у модели есть три метода, которые можно спутать друг с другом:\n",
        " - model.fit(...) - запуск обучения модели путем минимизации заданной ошибки заданным алгоритмом (на вход подаются x_train и y_train);\n",
        " - model.evaluate(...) - проверка качества обученной модели (обучения не происходит) (на вход подаются x_test и y_test);\n",
        " - model.predict(...) - получение предсказания модели на новых данных (на вход подается только x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYekHqBhxgc1",
        "colab_type": "text"
      },
      "source": [
        "### Задание на Keras\n",
        "\n",
        "Реализовать на Keras любую из ранее написанных нами нейросетей (можно даже логистическую регрессию). В качестве входных данных можно взять картинки с коробочками или набор точек из \"лепестков\".\n",
        "\n",
        "Добавить в нейросеть регуляризатор dropout.\n",
        "\n",
        "Воспользоваться оптимизатором adam.\n",
        "\n",
        "**Обратите внимание!** Если вы добавляете регуляризатор, то надо решить, до или после активации его применять. Чтобы применить регуляризатор до активации, необходимо НЕ передавать аргумент activation при добавлении слоя Dense. Активацию в таком случае надо добавить как слой сразу после dropout-a: keras.layers.Activation(activation=функция_активации)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfzeEAGv-qLn",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "PyTorch - еще один фреймворк глубокого обучения. Рассмотрим пример аппроксимации некоторой непрерывной функции с помощью двуслойной нейросети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guGm02Rfpq5_",
        "colab_type": "code",
        "outputId": "e2b3f03e-1366-434c-d732-f4d255cbfc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out)\n",
        "    \n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    print(t, loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 617.1824340820312\n",
            "1 600.6806640625\n",
            "2 584.6707153320312\n",
            "3 569.0603637695312\n",
            "4 553.8275756835938\n",
            "5 539.0787963867188\n",
            "6 524.8614501953125\n",
            "7 511.08837890625\n",
            "8 497.6931457519531\n",
            "9 484.7503662109375\n",
            "10 472.2181091308594\n",
            "11 460.0879821777344\n",
            "12 448.3591613769531\n",
            "13 437.0228576660156\n",
            "14 425.98846435546875\n",
            "15 415.3009338378906\n",
            "16 404.9441223144531\n",
            "17 394.8824462890625\n",
            "18 385.04730224609375\n",
            "19 375.5010986328125\n",
            "20 366.2279052734375\n",
            "21 357.1788635253906\n",
            "22 348.3672180175781\n",
            "23 339.82574462890625\n",
            "24 331.5387878417969\n",
            "25 323.49560546875\n",
            "26 315.62127685546875\n",
            "27 307.9244689941406\n",
            "28 300.40338134765625\n",
            "29 293.0827331542969\n",
            "30 285.9368591308594\n",
            "31 279.0050964355469\n",
            "32 272.2467346191406\n",
            "33 265.6567687988281\n",
            "34 259.2179870605469\n",
            "35 252.9234619140625\n",
            "36 246.79412841796875\n",
            "37 240.79286193847656\n",
            "38 234.91864013671875\n",
            "39 229.15802001953125\n",
            "40 223.5157470703125\n",
            "41 217.9838104248047\n",
            "42 212.59193420410156\n",
            "43 207.30865478515625\n",
            "44 202.126220703125\n",
            "45 197.05899047851562\n",
            "46 192.1124725341797\n",
            "47 187.2628936767578\n",
            "48 182.5348358154297\n",
            "49 177.8988800048828\n",
            "50 173.3536834716797\n",
            "51 168.9279022216797\n",
            "52 164.60946655273438\n",
            "53 160.38265991210938\n",
            "54 156.23541259765625\n",
            "55 152.17369079589844\n",
            "56 148.1888427734375\n",
            "57 144.2911376953125\n",
            "58 140.48304748535156\n",
            "59 136.75588989257812\n",
            "60 133.11073303222656\n",
            "61 129.54751586914062\n",
            "62 126.06597137451172\n",
            "63 122.66084289550781\n",
            "64 119.31897735595703\n",
            "65 116.05980682373047\n",
            "66 112.88253784179688\n",
            "67 109.76962280273438\n",
            "68 106.73243713378906\n",
            "69 103.76809692382812\n",
            "70 100.87024688720703\n",
            "71 98.03782653808594\n",
            "72 95.27731323242188\n",
            "73 92.58924102783203\n",
            "74 89.9570083618164\n",
            "75 87.39523315429688\n",
            "76 84.89901733398438\n",
            "77 82.45976257324219\n",
            "78 80.07501983642578\n",
            "79 77.75184631347656\n",
            "80 75.47757720947266\n",
            "81 73.2570571899414\n",
            "82 71.08965301513672\n",
            "83 68.97233581542969\n",
            "84 66.9050064086914\n",
            "85 64.89380645751953\n",
            "86 62.92571258544922\n",
            "87 61.00273513793945\n",
            "88 59.12947463989258\n",
            "89 57.29750061035156\n",
            "90 55.513343811035156\n",
            "91 53.777523040771484\n",
            "92 52.08591842651367\n",
            "93 50.43769836425781\n",
            "94 48.83586883544922\n",
            "95 47.277801513671875\n",
            "96 45.75826644897461\n",
            "97 44.279815673828125\n",
            "98 42.84294891357422\n",
            "99 41.440372467041016\n",
            "100 40.07583999633789\n",
            "101 38.74959945678711\n",
            "102 37.458839416503906\n",
            "103 36.20447540283203\n",
            "104 34.984962463378906\n",
            "105 33.800498962402344\n",
            "106 32.64912033081055\n",
            "107 31.530323028564453\n",
            "108 30.44285011291504\n",
            "109 29.38697052001953\n",
            "110 28.360198974609375\n",
            "111 27.36356544494629\n",
            "112 26.395965576171875\n",
            "113 25.457347869873047\n",
            "114 24.54741859436035\n",
            "115 23.666475296020508\n",
            "116 22.81095314025879\n",
            "117 21.98223876953125\n",
            "118 21.177867889404297\n",
            "119 20.39969825744629\n",
            "120 19.64600944519043\n",
            "121 18.915782928466797\n",
            "122 18.207149505615234\n",
            "123 17.522058486938477\n",
            "124 16.857728958129883\n",
            "125 16.21588897705078\n",
            "126 15.595190048217773\n",
            "127 14.994439125061035\n",
            "128 14.412691116333008\n",
            "129 13.84909439086914\n",
            "130 13.3042631149292\n",
            "131 12.776894569396973\n",
            "132 12.266849517822266\n",
            "133 11.774116516113281\n",
            "134 11.296932220458984\n",
            "135 10.836233139038086\n",
            "136 10.39064884185791\n",
            "137 9.960281372070312\n",
            "138 9.545329093933105\n",
            "139 9.145244598388672\n",
            "140 8.759252548217773\n",
            "141 8.386736869812012\n",
            "142 8.02785873413086\n",
            "143 7.6818389892578125\n",
            "144 7.348475456237793\n",
            "145 7.027523040771484\n",
            "146 6.718159198760986\n",
            "147 6.420486927032471\n",
            "148 6.134110927581787\n",
            "149 5.858894348144531\n",
            "150 5.594261169433594\n",
            "151 5.339602947235107\n",
            "152 5.095417022705078\n",
            "153 4.860571384429932\n",
            "154 4.635348796844482\n",
            "155 4.4193315505981445\n",
            "156 4.212400913238525\n",
            "157 4.013840675354004\n",
            "158 3.823641061782837\n",
            "159 3.64109468460083\n",
            "160 3.462815761566162\n",
            "161 3.289527654647827\n",
            "162 3.1215028762817383\n",
            "163 2.9590260982513428\n",
            "164 2.8023369312286377\n",
            "165 2.6516945362091064\n",
            "166 2.5072734355926514\n",
            "167 2.369091510772705\n",
            "168 2.237137794494629\n",
            "169 2.111180305480957\n",
            "170 1.991203784942627\n",
            "171 1.8773400783538818\n",
            "172 1.76915442943573\n",
            "173 1.666611909866333\n",
            "174 1.5696569681167603\n",
            "175 1.477920651435852\n",
            "176 1.3912746906280518\n",
            "177 1.3095711469650269\n",
            "178 1.2325578927993774\n",
            "179 1.1599000692367554\n",
            "180 1.091513991355896\n",
            "181 1.0271536111831665\n",
            "182 0.966550886631012\n",
            "183 0.909591794013977\n",
            "184 0.8559303879737854\n",
            "185 0.8055096864700317\n",
            "186 0.7580103278160095\n",
            "187 0.7134249210357666\n",
            "188 0.6714551448822021\n",
            "189 0.6319776773452759\n",
            "190 0.5948300361633301\n",
            "191 0.5599048137664795\n",
            "192 0.5270366668701172\n",
            "193 0.49612852931022644\n",
            "194 0.4670088291168213\n",
            "195 0.43962621688842773\n",
            "196 0.4138413667678833\n",
            "197 0.38957202434539795\n",
            "198 0.3667173385620117\n",
            "199 0.34519079327583313\n",
            "200 0.3249402940273285\n",
            "201 0.3058566153049469\n",
            "202 0.2878815829753876\n",
            "203 0.2709648311138153\n",
            "204 0.25504282116889954\n",
            "205 0.2400667816400528\n",
            "206 0.22595901787281036\n",
            "207 0.21267983317375183\n",
            "208 0.2001717984676361\n",
            "209 0.18841198086738586\n",
            "210 0.17733371257781982\n",
            "211 0.16691340506076813\n",
            "212 0.157107412815094\n",
            "213 0.14788609743118286\n",
            "214 0.13921217620372772\n",
            "215 0.1310501992702484\n",
            "216 0.12337417900562286\n",
            "217 0.1161571815609932\n",
            "218 0.10937398672103882\n",
            "219 0.10299817472696304\n",
            "220 0.09701580554246902\n",
            "221 0.09137417376041412\n",
            "222 0.08608029782772064\n",
            "223 0.081102654337883\n",
            "224 0.07642702013254166\n",
            "225 0.07203742861747742\n",
            "226 0.06790382415056229\n",
            "227 0.06402163952589035\n",
            "228 0.06037196144461632\n",
            "229 0.05694086104631424\n",
            "230 0.05371437221765518\n",
            "231 0.050679776817560196\n",
            "232 0.047825444489717484\n",
            "233 0.045140817761421204\n",
            "234 0.04261299595236778\n",
            "235 0.04023433104157448\n",
            "236 0.037994883954524994\n",
            "237 0.035887058824300766\n",
            "238 0.033901337534189224\n",
            "239 0.03203140199184418\n",
            "240 0.030268866568803787\n",
            "241 0.02860700711607933\n",
            "242 0.027041422203183174\n",
            "243 0.025565199553966522\n",
            "244 0.024172501638531685\n",
            "245 0.022858712822198868\n",
            "246 0.021619396284222603\n",
            "247 0.0204489566385746\n",
            "248 0.01934470795094967\n",
            "249 0.018302014097571373\n",
            "250 0.017317600548267365\n",
            "251 0.01638755388557911\n",
            "252 0.015509050339460373\n",
            "253 0.01467902772128582\n",
            "254 0.013894621282815933\n",
            "255 0.013153476640582085\n",
            "256 0.012452969327569008\n",
            "257 0.011790025047957897\n",
            "258 0.011163552291691303\n",
            "259 0.010571205988526344\n",
            "260 0.01001061499118805\n",
            "261 0.009480462409555912\n",
            "262 0.008978815749287605\n",
            "263 0.008504020050168037\n",
            "264 0.008054745383560658\n",
            "265 0.0076294573955237865\n",
            "266 0.007227017544209957\n",
            "267 0.006846104748547077\n",
            "268 0.006484957877546549\n",
            "269 0.0061433142982423306\n",
            "270 0.005819766782224178\n",
            "271 0.005513499025255442\n",
            "272 0.0052230809815227985\n",
            "273 0.0049481699243187904\n",
            "274 0.004687712527811527\n",
            "275 0.004440802149474621\n",
            "276 0.004206981975585222\n",
            "277 0.0039852992631495\n",
            "278 0.003775411518290639\n",
            "279 0.003576345043256879\n",
            "280 0.003387743141502142\n",
            "281 0.0032089930027723312\n",
            "282 0.003039540257304907\n",
            "283 0.0028789767529815435\n",
            "284 0.0027267092373222113\n",
            "285 0.0025824159383773804\n",
            "286 0.0024456139653921127\n",
            "287 0.002315942430868745\n",
            "288 0.00219303322955966\n",
            "289 0.0020765389781445265\n",
            "290 0.0019660869147628546\n",
            "291 0.0018614004366099834\n",
            "292 0.001762180239893496\n",
            "293 0.0016683636931702495\n",
            "294 0.0015790086472406983\n",
            "295 0.001494516502134502\n",
            "296 0.0014144766610115767\n",
            "297 0.0013385836500674486\n",
            "298 0.001266677281819284\n",
            "299 0.0011985276360064745\n",
            "300 0.0011339426273480058\n",
            "301 0.001072732382453978\n",
            "302 0.0010147800203412771\n",
            "303 0.0009598344913683832\n",
            "304 0.0009077672148123384\n",
            "305 0.0008584632887504995\n",
            "306 0.0008117866818793118\n",
            "307 0.0007675582892261446\n",
            "308 0.000725660880561918\n",
            "309 0.0006859663408249617\n",
            "310 0.0006483898032456636\n",
            "311 0.000612816249486059\n",
            "312 0.0005791351431980729\n",
            "313 0.0005472355987876654\n",
            "314 0.0005170399672351778\n",
            "315 0.000488452787976712\n",
            "316 0.00046140135964378715\n",
            "317 0.0004357955476734787\n",
            "318 0.00041155918734148145\n",
            "319 0.00038863596273586154\n",
            "320 0.00036693669972009957\n",
            "321 0.0003464095061644912\n",
            "322 0.00032699696021154523\n",
            "323 0.0003086342476308346\n",
            "324 0.00029126222943887115\n",
            "325 0.0002748437982518226\n",
            "326 0.00025930505944415927\n",
            "327 0.00024461784050799906\n",
            "328 0.0002307402901351452\n",
            "329 0.00021761583047918975\n",
            "330 0.00020522075647022575\n",
            "331 0.00019349661306478083\n",
            "332 0.0001824179635150358\n",
            "333 0.0001719590072752908\n",
            "334 0.00016206986038014293\n",
            "335 0.000152734704897739\n",
            "336 0.00014391914010047913\n",
            "337 0.00013558912905864418\n",
            "338 0.000127719875308685\n",
            "339 0.00012030128709739074\n",
            "340 0.00011329493281664327\n",
            "341 0.0001066805052687414\n",
            "342 0.00010043520160252228\n",
            "343 9.454330574953929e-05\n",
            "344 8.898792293621227e-05\n",
            "345 8.374386379728094e-05\n",
            "346 7.879912300268188e-05\n",
            "347 7.413043203996494e-05\n",
            "348 6.97355717420578e-05\n",
            "349 6.559079338330775e-05\n",
            "350 6.167710671434179e-05\n",
            "351 5.799349310109392e-05\n",
            "352 5.452246841741726e-05\n",
            "353 5.1247588999103755e-05\n",
            "354 4.816522778128274e-05\n",
            "355 4.525869371718727e-05\n",
            "356 4.252446160535328e-05\n",
            "357 3.99463351641316e-05\n",
            "358 3.752065822482109e-05\n",
            "359 3.523531268001534e-05\n",
            "360 3.3085860195569694e-05\n",
            "361 3.106255462625995e-05\n",
            "362 2.9156935852370225e-05\n",
            "363 2.7361844331608154e-05\n",
            "364 2.5677840312710032e-05\n",
            "365 2.4090451915981248e-05\n",
            "366 2.259853863506578e-05\n",
            "367 2.1196179659455083e-05\n",
            "368 1.9876568330801092e-05\n",
            "369 1.8635493688634597e-05\n",
            "370 1.7467897123424336e-05\n",
            "371 1.6374915503547527e-05\n",
            "372 1.534508373879362e-05\n",
            "373 1.4377539628185332e-05\n",
            "374 1.3469124496623408e-05\n",
            "375 1.2615521882253233e-05\n",
            "376 1.1815270227089059e-05\n",
            "377 1.1062717931054067e-05\n",
            "378 1.0356078746553976e-05\n",
            "379 9.69310804066481e-06\n",
            "380 9.072350621863734e-06\n",
            "381 8.487116247124504e-06\n",
            "382 7.940210707602091e-06\n",
            "383 7.426882802974433e-06\n",
            "384 6.945575933059445e-06\n",
            "385 6.49450657874695e-06\n",
            "386 6.070054951123893e-06\n",
            "387 5.675573447661009e-06\n",
            "388 5.302726549416548e-06\n",
            "389 4.954723408445716e-06\n",
            "390 4.628102487913566e-06\n",
            "391 4.322869244788308e-06\n",
            "392 4.037231519760098e-06\n",
            "393 3.7690494991693413e-06\n",
            "394 3.517971435940126e-06\n",
            "395 3.28303690366738e-06\n",
            "396 3.0642097499367082e-06\n",
            "397 2.8584001938725123e-06\n",
            "398 2.666218961167033e-06\n",
            "399 2.486945959390141e-06\n",
            "400 2.318331098649651e-06\n",
            "401 2.1618197934003547e-06\n",
            "402 2.014607161981985e-06\n",
            "403 1.877753220469458e-06\n",
            "404 1.7497412727607298e-06\n",
            "405 1.6294558236040757e-06\n",
            "406 1.5177163277257932e-06\n",
            "407 1.4128090697340667e-06\n",
            "408 1.315958002123807e-06\n",
            "409 1.2244282743267831e-06\n",
            "410 1.1397303296689643e-06\n",
            "411 1.060406816577597e-06\n",
            "412 9.8684301974572e-07\n",
            "413 9.176084745377011e-07\n",
            "414 8.530091690772679e-07\n",
            "415 7.931174650366302e-07\n",
            "416 7.371718879767286e-07\n",
            "417 6.848204066045582e-07\n",
            "418 6.362644171531429e-07\n",
            "419 5.912192477808276e-07\n",
            "420 5.49344861155987e-07\n",
            "421 5.100054067952442e-07\n",
            "422 4.7337076125586464e-07\n",
            "423 4.3953207295999164e-07\n",
            "424 4.0788916066958336e-07\n",
            "425 3.782583348765911e-07\n",
            "426 3.5105162510262744e-07\n",
            "427 3.2533756666452973e-07\n",
            "428 3.01790379353406e-07\n",
            "429 2.79988540796694e-07\n",
            "430 2.5958300398087886e-07\n",
            "431 2.4015875510485785e-07\n",
            "432 2.2280286771092506e-07\n",
            "433 2.0647280507546384e-07\n",
            "434 1.9121408456612699e-07\n",
            "435 1.7711496980155061e-07\n",
            "436 1.6372281663734611e-07\n",
            "437 1.5158306609919237e-07\n",
            "438 1.4032661965757143e-07\n",
            "439 1.299197265325347e-07\n",
            "440 1.200159687186897e-07\n",
            "441 1.1103603014817054e-07\n",
            "442 1.0278593265411473e-07\n",
            "443 9.494931418885244e-08\n",
            "444 8.776452631309439e-08\n",
            "445 8.110781379855325e-08\n",
            "446 7.499725995785411e-08\n",
            "447 6.925290563231101e-08\n",
            "448 6.401823071655599e-08\n",
            "449 5.902521493794666e-08\n",
            "450 5.4536389626491655e-08\n",
            "451 5.028105931614846e-08\n",
            "452 4.6498037420406035e-08\n",
            "453 4.289064037266144e-08\n",
            "454 3.956958138928712e-08\n",
            "455 3.6540569681164925e-08\n",
            "456 3.370472256847279e-08\n",
            "457 3.105720125518019e-08\n",
            "458 2.869046156206423e-08\n",
            "459 2.6421982823876533e-08\n",
            "460 2.439692536881921e-08\n",
            "461 2.2417227185655975e-08\n",
            "462 2.0656839794241932e-08\n",
            "463 1.9034191112154986e-08\n",
            "464 1.7553393405478346e-08\n",
            "465 1.613721423154857e-08\n",
            "466 1.4896537336994697e-08\n",
            "467 1.3715049540508062e-08\n",
            "468 1.2647176639291047e-08\n",
            "469 1.1618784157008122e-08\n",
            "470 1.0711022291332029e-08\n",
            "471 9.86553239101795e-09\n",
            "472 9.103131581866819e-09\n",
            "473 8.359350545106281e-09\n",
            "474 7.708833571484774e-09\n",
            "475 7.122970213657709e-09\n",
            "476 6.5288370265648155e-09\n",
            "477 6.013924913617075e-09\n",
            "478 5.5882924954175905e-09\n",
            "479 5.1452708760280075e-09\n",
            "480 4.7443338146990754e-09\n",
            "481 4.377713302972097e-09\n",
            "482 4.0506584753075e-09\n",
            "483 3.719878627350681e-09\n",
            "484 3.4445875041200225e-09\n",
            "485 3.175444351910528e-09\n",
            "486 2.933524312354052e-09\n",
            "487 2.7025390814117145e-09\n",
            "488 2.4873165749284e-09\n",
            "489 2.2970612079120656e-09\n",
            "490 2.141190114102187e-09\n",
            "491 1.9934323081116645e-09\n",
            "492 1.8417635194722948e-09\n",
            "493 1.7130040719237627e-09\n",
            "494 1.5861291169372294e-09\n",
            "495 1.4550086691045294e-09\n",
            "496 1.3607358573253237e-09\n",
            "497 1.2453514885990558e-09\n",
            "498 1.1711466241237645e-09\n",
            "499 1.0835615738002957e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmye5H7OEymw",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "На этом уроке мы познакомились с тремя фреймворками для deep-learning:\n",
        " - TensorFlow - узнали порядок написания программ, узнали типы данных Variable и placeholder;\n",
        " - Keras (который не совсем фреймворк, а скорее интерфейс) - написали в нем простую нейросеть;\n",
        " - PyTorch - аналогично написали простую двуслойную нейросеть и посмотрели, как ее обучать.\n",
        "\n",
        "Помимо этого, мы рассмотрели задачу многоклассовой классификации, а именно:\n",
        " - узнали, что такое функция Softmax;\n",
        " - узнали, что такое one-hot encoding и как его реализовывать на tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ixtWxreEFXZ",
        "colab_type": "code",
        "outputId": "af56ba63-3ebf-4825-a7f6-47b712086766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 22 10:26:44 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    68W / 149W |    153MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj7dg8BDjI4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}