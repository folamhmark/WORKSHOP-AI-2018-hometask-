{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/folamhmark/WORKSHOP-AI-2019-hometask-/blob/master/Lesson1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYdM-UszD1wa",
        "colab_type": "code",
        "outputId": "62322b36-48fb-4f25-883b-8c0494c921f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "path = './gdrive/My Drive/ds/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT25YwXnEC-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "X = None\n",
        "Y = np.array([])\n",
        "\n",
        "def read_files(X, Y, path, ans):\n",
        "  files = os.listdir(path)\n",
        "  for name in files:\n",
        "    img = cv2.imread(path + '/' + name, 0)\n",
        "    if img.shape != 0:\n",
        "      img = cv2.resize(img, (256, 256))\n",
        "      vect = img.reshape(1, 256 ** 2)\n",
        "      vect = vect / 255.\n",
        "      X = vect if (X is None) else np.vstack((X, vect)) \n",
        "      Y = np.append(Y, ans)\n",
        "  return X, Y\n",
        "X_0,Y_0=read_files(X,Y,path+'logloss_1',1);\n",
        "X_1,Y_1=read_files(X,Y,path+'logloss_0',0);\n",
        "X = np.concatenate([X_0, X_1])\n",
        "Y = np.concatenate([Y_0, Y_1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mra6mBViFbIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=48)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu3W4w_LExck",
        "colab_type": "code",
        "outputId": "390e9e1b-5865-4e54-b8a5-e3f2024bfaa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def init_model(input_size=256):\n",
        "    ###ЗАДАЧА: проинициализируйте веса модели так, чтобы массив w имел размер (input_size^2,1), а b был числом\n",
        "    w = np.array([0] * (input_size ** 2))\n",
        "    b = 0\n",
        "    return w, b\n",
        "\n",
        "def sigmoid(z):\n",
        "  ###ВАЖНО: функция принимает на вход массив любых размеров, на выход возвращает массив такого же размера\n",
        "  return 1.0/ (1 + np.exp(-z))\n",
        "   \n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    w, X, Y = np.array(w), np.array(X), np.array(Y)\n",
        "    \"\"\"\n",
        "    Подсчет текущего предсказания (оно же forward propagation) и градиента функции ошибки (оно же backward propagation)\n",
        "\n",
        "    Input:\n",
        "    w -- веса, numpy_array размера (num_px * num_px * 3, 1)\n",
        "    b -- смещение, скалярная величина\n",
        "    X -- данные размера (num_px * num_px * 3, кол-во образцов)\n",
        "    Y -- вектор истинных ответов размера (1, кол-во образцов)\n",
        "\n",
        "    Return:\n",
        "    cost -- текущая функция потерь\n",
        "    dw -- градиент функции ошибки по w\n",
        "    db -- градиент функции ошибки по b (по сути производная по b)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "          \n",
        "    \n",
        "    A = sigmoid(np.dot(w.T,X.T) + b)\n",
        "    \n",
        "    cost = -1.0 / m * np.sum(Y * np.log(A) + (1.0 - Y) * np.log(1.0 - A))\n",
        "      \n",
        "    dw = 1 / m *(np.dot(X.T,(A - Y).T))\n",
        "    db = 1 / m *(np.sum(A - Y))\n",
        "        \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    Оптимизация с помощью простого градиентного спуска\n",
        "    \n",
        "    Input:\n",
        "    w -- веса, numpy_array размера (num_px * num_px * 3, 1)\n",
        "    b -- смещение, скалярная величина\n",
        "    X -- данные размера (num_px * num_px * 3, кол-во образцов)\n",
        "    Y -- вектор истинных ответов размера (1, кол-во образцов)\n",
        "    num_iterations -- кол-во итераций алгоритма оптимизации\n",
        "    learning_rate -- коэффициент learning rate\n",
        "    print_cost -- True, если хотите выводить функцию ошибки на каждых 100 итерациях\n",
        "    \n",
        "    Returns:\n",
        "    params -- словарь, содержащий w и b\n",
        "    grads -- словарь, содержащий градиенты функции ошибки по w и b соответственно\n",
        "    costs -- массив (list) со значением функции ошибки для каждой итерации (так делают для визуализации)\n",
        "    \n",
        "    Подсказка:\n",
        "    \n",
        "        1) Используйте ранее написанную функцию propagate().\n",
        "        2) Обновляйте параметры w и b согласно формуле 10.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        ###Напишите значения для градиентов и функции ошибки\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        \n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # обновление параметров\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        w = w-learning_rate*dw\n",
        "        b = b-learning_rate*db\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "  \n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    \n",
        "    Inputs:\n",
        "    w\n",
        "    b\n",
        "    X -- данные размера (num_px * num_px * 3, кол-во образцов)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction\n",
        "    '''\n",
        "    m = X.shape[0]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.T.shape[0], 1)\n",
        "    \n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    A = sigmoid(np.dot(w.T,X.T) + b)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Установите порог, выше которого считаем, что модель выдает 1, а ниже - ноль\n",
        "        ### START CODE HERE ###\n",
        "        if(A[:, i] <= 0.5):\n",
        "            Y_prediction[:, i] = 0\n",
        "        else:\n",
        "            Y_prediction[:, i] = 1\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    return Y_prediction\n",
        "\n",
        "w,b=init_model()\n",
        " \n",
        "\n",
        "parameters, grads, costs= optimize(w, b, X_train, Y_train, 10000, 0.005, print_cost = True)\n",
        "\n",
        "w = parameters[\"w\"]\n",
        "b = parameters[\"b\"]\n",
        "\n",
        "\n",
        "y_train = predict(w,b,np.array(X_train))\n",
        "y_test = predict(w, b, X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "train_accuracy = accuracy_score(Y_train,y_train[0])\n",
        "test_accuracy = accuracy_score(Y_test,y_test[0])\n",
        "print(train_accuracy)\n",
        "print(test_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.000476\n",
            "Cost after iteration 100: 0.000464\n",
            "Cost after iteration 200: 0.000456\n",
            "Cost after iteration 300: 0.000450\n",
            "Cost after iteration 400: 0.000446\n",
            "Cost after iteration 500: 0.000442\n",
            "Cost after iteration 600: 0.000438\n",
            "Cost after iteration 700: 0.000434\n",
            "Cost after iteration 800: 0.000431\n",
            "Cost after iteration 900: 0.000428\n",
            "Cost after iteration 1000: 0.000425\n",
            "Cost after iteration 1100: 0.000422\n",
            "Cost after iteration 1200: 0.000418\n",
            "Cost after iteration 1300: 0.000416\n",
            "Cost after iteration 1400: 0.000413\n",
            "Cost after iteration 1500: 0.000410\n",
            "Cost after iteration 1600: 0.000407\n",
            "Cost after iteration 1700: 0.000404\n",
            "Cost after iteration 1800: 0.000402\n",
            "Cost after iteration 1900: 0.000399\n",
            "Cost after iteration 2000: 0.000396\n",
            "Cost after iteration 2100: 0.000394\n",
            "Cost after iteration 2200: 0.000391\n",
            "Cost after iteration 2300: 0.000389\n",
            "Cost after iteration 2400: 0.000386\n",
            "Cost after iteration 2500: 0.000384\n",
            "Cost after iteration 2600: 0.000382\n",
            "Cost after iteration 2700: 0.000379\n",
            "Cost after iteration 2800: 0.000377\n",
            "Cost after iteration 2900: 0.000375\n",
            "Cost after iteration 3000: 0.000372\n",
            "Cost after iteration 3100: 0.000370\n",
            "Cost after iteration 3200: 0.000368\n",
            "Cost after iteration 3300: 0.000366\n",
            "Cost after iteration 3400: 0.000364\n",
            "Cost after iteration 3500: 0.000362\n",
            "Cost after iteration 3600: 0.000360\n",
            "Cost after iteration 3700: 0.000358\n",
            "Cost after iteration 3800: 0.000356\n",
            "Cost after iteration 3900: 0.000354\n",
            "Cost after iteration 4000: 0.000352\n",
            "Cost after iteration 4100: 0.000350\n",
            "Cost after iteration 4200: 0.000348\n",
            "Cost after iteration 4300: 0.000346\n",
            "Cost after iteration 4400: 0.000345\n",
            "Cost after iteration 4500: 0.000343\n",
            "Cost after iteration 4600: 0.000341\n",
            "Cost after iteration 4700: 0.000339\n",
            "Cost after iteration 4800: 0.000338\n",
            "Cost after iteration 4900: 0.000336\n",
            "Cost after iteration 5000: 0.000334\n",
            "Cost after iteration 5100: 0.000332\n",
            "Cost after iteration 5200: 0.000331\n",
            "Cost after iteration 5300: 0.000329\n",
            "Cost after iteration 5400: 0.000328\n",
            "Cost after iteration 5500: 0.000326\n",
            "Cost after iteration 5600: 0.000324\n",
            "Cost after iteration 5700: 0.000323\n",
            "Cost after iteration 5800: 0.000321\n",
            "Cost after iteration 5900: 0.000320\n",
            "Cost after iteration 6000: 0.000318\n",
            "Cost after iteration 6100: 0.000317\n",
            "Cost after iteration 6200: 0.000315\n",
            "Cost after iteration 6300: 0.000314\n",
            "Cost after iteration 6400: 0.000312\n",
            "Cost after iteration 6500: 0.000311\n",
            "Cost after iteration 6600: 0.000310\n",
            "Cost after iteration 6700: 0.000308\n",
            "Cost after iteration 6800: 0.000307\n",
            "Cost after iteration 6900: 0.000305\n",
            "Cost after iteration 7000: 0.000304\n",
            "Cost after iteration 7100: 0.000303\n",
            "Cost after iteration 7200: 0.000301\n",
            "Cost after iteration 7300: 0.000300\n",
            "Cost after iteration 7400: 0.000299\n",
            "Cost after iteration 7500: 0.000298\n",
            "Cost after iteration 7600: 0.000296\n",
            "Cost after iteration 7700: 0.000295\n",
            "Cost after iteration 7800: 0.000294\n",
            "Cost after iteration 7900: 0.000293\n",
            "Cost after iteration 8000: 0.000291\n",
            "Cost after iteration 8100: 0.000290\n",
            "Cost after iteration 8200: 0.000289\n",
            "Cost after iteration 8300: 0.000288\n",
            "Cost after iteration 8400: 0.000287\n",
            "Cost after iteration 8500: 0.000285\n",
            "Cost after iteration 8600: 0.000284\n",
            "Cost after iteration 8700: 0.000283\n",
            "Cost after iteration 8800: 0.000282\n",
            "Cost after iteration 8900: 0.000281\n",
            "Cost after iteration 9000: 0.000280\n",
            "Cost after iteration 9100: 0.000279\n",
            "Cost after iteration 9200: 0.000278\n",
            "Cost after iteration 9300: 0.000276\n",
            "Cost after iteration 9400: 0.000275\n",
            "Cost after iteration 9500: 0.000274\n",
            "Cost after iteration 9600: 0.000273\n",
            "Cost after iteration 9700: 0.000272\n",
            "Cost after iteration 9800: 0.000271\n",
            "Cost after iteration 9900: 0.000270\n",
            "1.0\n",
            "0.9\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}