{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/folamhmark/WORKSHOP-AI-2019-hometask-/blob/master/Lesson_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtWyAAjZDlPv",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning\n",
        "# Урок 3\n",
        "\n",
        "## Часть 1\n",
        "\n",
        "На прошлом занятии мы построили двухслойную нейросеть (она же нейросеть с одним скрытым слоем) для задачи бинарной классификации (мы предсказывали цвет точки из двумерного пространства в зависимости от ее координат). По сравнению с логистической регрессией качество существенно возросло. \n",
        "\n",
        "Тем не менее, такая модель не подходит для более сложных задач или более сложного распределения данных. На этом уроке мы будем строить n-слойные нейросети. Перед этим вспомним формулы прошлого урока:\n",
        "\n",
        "$\\textbf{Forward propagation:}$\n",
        "\n",
        "$$ z^{[1]} = W^{[1]}\\cdot x + b^{[1]} \\tag{1.1}$$\n",
        "\n",
        "$$ a^{[1]} = \\sigma^{[1]} (z^{[1]}) \\tag{1.2}$$\n",
        "\n",
        "$$ z^{[2]} = W^{[2]} \\cdot a^{[1]} + b^{[2]} \\tag{1.3} $$\n",
        "\n",
        "$$ a^{[2]} = \\sigma^{[2]} (z^{[2]}) \\tag{1.4} $$\n",
        "\n",
        "$$ J(a^{[2]}, y) = ... \\tag{1.5}$$\n",
        "\n",
        "$$ y = threshold(a^{[2]}). \\tag{1.6}$$\n",
        "\n",
        "$\\textbf{Backward propagation:}$\n",
        "\n",
        "Минимизируем следующую функцию ошибки:\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{2.1}$$\n",
        "\n",
        "\n",
        "$$dz^{[2]} = a^{[2]}-y \\tag{2.2}$$\n",
        "\n",
        "$$dw^{[2]} = \\frac{\\partial J}{\\partial z^{[2]}} \\cdot \\frac{\\partial z^{[2]}}{\\partial w^{[2]}}  = \\frac{1}{m}\\cdot dz^{[2]} \\cdot a^{[1]T} \\tag{2.3}$$\n",
        "\n",
        "$$db_j^{[2]} = \\frac{1}{m} \\sum_{i=1}^{m} {dz_{j}^{[2](i)}} ,  j = \\{1\\} \\tag{2.4}$$\n",
        "\n",
        "$$dz^{[1]} =  \\frac{\\partial J}{\\partial a^{[1]}} * \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} = w^{[2]T} \\cdot dz^{[2]}* \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\tag{2.5}$$\n",
        "\n",
        "$$dw^{[1]} = \\frac{\\partial J}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial w^{[1]}} =  \\frac{1}{m} \\cdot dz^{[1]} \\cdot x^{T}\\tag{2.6}$$\n",
        "\n",
        "$$db_{j}^{[1]} =\\frac{1}{m}\\sum_{i=1}^m {dz_j^{[1](i)}} , j =\\{1,2,3,4\\} \\tag{2.7} $$\n",
        "\n",
        "Как описывать forward- и backward propagation в многослойных нейросетях, мы сейчас узнаем..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2p60pkyIJfy",
        "colab_type": "text"
      },
      "source": [
        "## Часть 2\n",
        "\n",
        "Нейросеть, с которой мы сегодня будем работать, имеет следующую архитектуру:\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=109x3ZBbzo520i4oup-BoFtZuKJKfNsOi\" style=\"width:665px;height:320px;\">\n",
        "\n",
        "В качестве входных данных снова будем использовать изображения, в качестве вывода - метку 0 или 1.\n",
        "\n",
        "Есть статьи, которые описывают классы функций, которые аппроксимируются глубокими нейросетями с той же точностью, что и неглубокими сетями с экспоненциально бОльшим кол-вом нейронов.\n",
        "\n",
        "Определимся с терминологией: в литературе глубокие нейросети (2 и более скрытых слоя) называются $\\textit{Deep neural networks}$, неглубокие (1 скрытый слой) - $\\textit{Shallow neural networks}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nQr7MPcqi61",
        "colab_type": "text"
      },
      "source": [
        "Для упрощения работы сразу объявим функции активации и функции, вычисляющие их производные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77PllBXeDb14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Вычисление сигмоидной функции.\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy_array\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Вычисление функции ReLU.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy_array\n",
        "\n",
        "    Returns:\n",
        "    A -- Применение ReLU к Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Backprop для функции ReLU.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    #print(\"Z shape is \",cache.shape)\n",
        "    #print(\"dA shape is\", dA.shape)\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    #print(cache.shape)\n",
        "    \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "   \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Backprop для сигмоиды.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbKfblD1fpa0",
        "colab_type": "code",
        "outputId": "11551627-3b5e-4a06-bb37-f3f84321ffc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "path = './gdrive/My Drive/ds/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqLuPKl5gJt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "X = None\n",
        "Y = np.array([])\n",
        "\n",
        "def read_files(X, Y, path, ans):\n",
        "  files = os.listdir(path)\n",
        "  for name in files:\n",
        "    img = cv2.imread(path + '/' + name, 0)\n",
        "    if img.shape != 0:\n",
        "      img = cv2.resize(img, (64, 64))\n",
        "      vect = img.reshape(1, 64 ** 2)\n",
        "      vect = vect / 255.\n",
        "      X = vect if (X is None) else np.vstack((X, vect)) \n",
        "      Y = np.append(Y, ans)\n",
        "  return X, Y\n",
        "X_0,Y_0=read_files(X,Y,path+'logloss_1',1);\n",
        "X_1,Y_1=read_files(X,Y,path+'logloss_0',0);\n",
        "X = np.concatenate([X_0, X_1])\n",
        "Y = np.concatenate([Y_0, Y_1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=48)\n",
        "X_train, X_test = X_train, X_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l1rl_83bpGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_planar_dataset():\n",
        "    np.random.seed(1)\n",
        "    m = 400 # number of examples\n",
        "    N = int(m/2) # number of points per class\n",
        "    D = 2 # dimensionality\n",
        "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
        "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
        "    a = 4 # maximum ray of the flower\n",
        "\n",
        "    for j in range(2):\n",
        "        ix = range(N*j,N*(j+1))\n",
        "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
        "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
        "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "        Y[ix] = j\n",
        "        \n",
        "    X = X.T\n",
        "    Y = Y.T\n",
        "\n",
        "    return X, Y\n",
        "  \n",
        "X, Y = load_planar_dataset() #загружаем датасет\n",
        "#print(X_train.shape)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=48)\n",
        "X_train, X_test = X_train.T, X_test.T\n",
        "#print(Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnpPERlIs63Z",
        "colab_type": "text"
      },
      "source": [
        "### Шаг 1. Инициализация модели\n",
        "\n",
        "Построение модели, как и в прошлые разы, мы начинаем с инициализации параметров. В многослойной нейросети необходимо для этого определить размеры каждого слоя. **REMINDER** Матрица весов l-ого слоя $w^{[l]}$ имеет размеры $(l,l-1)$.\n",
        "\n",
        "**Задание**: Инициализируйте параметры для L-слойной нейросети. \n",
        "\n",
        "**Инструкция**:\n",
        "- Модель выглядит следующим образом: *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. Т.е. в ней есть $L-1$ слоев с активацией ReLU, а последний слой имеет сигмоидную активацию.\n",
        "- Используйте генератор случайных чисел из нормального распределения для инициализации матриц весов $w$  (`np.random.randn(shape) * 0.01`).\n",
        "- Векторы смещения $b$ инициализируйте нулями (`np.zeros(shape)`).\n",
        "- В массиве layer_dims мы зададим размерность каждого слоя - i-ый элемент этого массиво является целым числом=размерности слоя."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ85QLgJ61by",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- массив с размерностями каждого слоя\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- матрица весов размера (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- вектор смещения размера (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        ### START CODE HERE ###\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])       \n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    #print(parameters.keys())\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2waGy-Qjn7b0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sm8Pljo7KrY",
        "colab_type": "text"
      },
      "source": [
        "### Шаг 2. Построение модуля forward_propagation\n",
        "\n",
        "Чтобы упростить написание кода, разобьём forward_propagation на три функции и напишем их последовательно:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION, где ACTIVATION - ReLU или Sigmoid. \n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (сборка всего блока forward_propagation)\n",
        "\n",
        "Функция LINEAR  проводит следующие вычисления:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]},\\tag{3}$$\n",
        "\n",
        "где $A^{[0]} = X$. \n",
        "\n",
        "**Задание**: опишите функцию LINEAR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tMJxEOUuqwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- вывод активации предыдущего слоя\n",
        "    W -- матрица весов: numpy_array размера (size of current layer, size of previous layer)\n",
        "    b -- вектор смещения, numpy_array размера (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    #A=A.T\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "\n",
        "    \n",
        "    Z = np.dot(W,A)+b\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    \n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ott2zpH_9f_j",
        "colab_type": "text"
      },
      "source": [
        "В нашей нейросети мы используем две различных функции активации:\n",
        "\n",
        "- **Sigmoid**: $sigmoid(Z) = sigmoid(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Эту функцию мы уже объявили (`sigmoid`). Функция возвращает  **два** значения: вывод активации \"`a`\" и \"`cache`\", который содержит \"`Z`\" (это потребуется для реализации back_propagation). Пример использования (тривиален): \n",
        "``` python\n",
        "A, activation_cache = sigmoid(Z)\n",
        "```\n",
        "\n",
        "- **ReLU**: Формула: $A = RELU(Z) = max(0, Z)$. Эту функцию мы также заранее объявили (`relu`) . Возвращает также два значения (как и sigmoid):\n",
        "``` python\n",
        "A, activation_cache = relu(Z)\n",
        "```\n",
        "\n",
        "**Задание**: Реализуйте блок *LINEAR->ACTIVATION*. Функция должна проводить следующие величины: $A^{[l]} = \\sigma(Z^{[l]}) = \\sigma(W^{[l]}A^{[l-1]} +b^{[l]})$, где $\\sigma$- либо sigmoid(), либо relu(). Используйте linear_forward() и соответствующую функцию активации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MizXiznW9XXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ###\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        \n",
        "        A, activation_cache = sigmoid(Z)\n",
        "        cache = (linear_cache, activation_cache)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        \n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "        cache = (linear_cache, activation_cache)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    \n",
        "    #cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMD350tc_oAt",
        "colab_type": "text"
      },
      "source": [
        "Отлично! С помощью двух предыдущих функций можно теперь собрать весь блок forward_propagation для L-слойной сети в одну функцию.\n",
        "\n",
        "**Задание**:  реализуйте шаг forward_propagation для данной сети: *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*. Не забудьте в массив caches положить текущий cache. (caches.append(cache))\n",
        "\n",
        "**Инструкция**: В коде ниже переменная `AL` обозначает $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTKdE2tAVfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    #print(L)\n",
        "    \n",
        "    # Выполните [LINEAR -> RELU]*(L-1) раз. Добавьте \"cache\" в массив \"caches\".\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        \n",
        "        A, cache = linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n",
        "        caches.append(cache)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    # Выполните LINEAR -> SIGMOID. Добавьте \"cache\" в массив \"caches\".\n",
        "    ### START CODE HERE ### \n",
        "    AL, cache = linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')\n",
        "    caches.append(cache)\n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6mhcqlJBSLn",
        "colab_type": "text"
      },
      "source": [
        "### Шаг 3. Подсчет функции потерь\n",
        "\n",
        "Поскольку мы решаем ту же самую задачу бинарной классификации, будем использовать ту же функцию потерь, что и в предыдущие разы (бинарную кросс-энтропию):\n",
        "\n",
        "$$J= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{4}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1hrFTZhBwRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "    #cost = -1 / m *(np.dot(Y,np.log(AL.T)) + np.dot(1 - Y,np.log(1 - AL).T))\n",
        "    \n",
        "    Y=Y.T\n",
        "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot((1-Y), np.log(1-AL).T))\n",
        "    \n",
        "    #print(AL.shape)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    #print(cost)\n",
        "    \n",
        "    cost = np.squeeze(np.squeeze(cost))      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "  \n",
        "    ### END CODE HERE ###\n",
        "    #print('is cost',cost)\n",
        "    #print('is m')\n",
        "    \n",
        "  \n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9GLiSBoCZHp",
        "colab_type": "text"
      },
      "source": [
        "### Шаг 4. Построение модуля back_propagation\n",
        "\n",
        "Для наглядности обратимся к данному рисунку:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1kmdNwswi7WNMJCf9aWuuy4ohbsEhQJ9h\" style=\"width:665px;height:320px;\">\n",
        "\n",
        "На каждом шаге forward_propagation мы сохраняли в массив caches значения вывода слоя до активации и после активации. Это было сделано, поскольку back_propagation нуждается в этих выводах.\n",
        "\n",
        "Точно так же будем проводить подсчет back_propagation в два этапа:\n",
        "    \n",
        "  1)  подсчет $dw^{[l]}$, $db^{[l]}$ и $da^{[l-1]}$ в предположении, что $dz^{[l]}$ уже известен\n",
        "    \n",
        "   2) подсчет $dz^{[l]}$.\n",
        "    \n",
        "Первый этап описывается следующими формулами:\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial w^{[l]}} = \\frac{1}{m} dz^{[l]} a^{[l-1] T} \\tag{5}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dz^{[l](i)}\\tag{6}$$\n",
        "$$ da^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[l-1]}} = w^{[l] T} dz^{[l]} \\tag{7}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANDt4L-aHw7P",
        "colab_type": "text"
      },
      "source": [
        "**Задание**: реализуйте первый этап модуля back_propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5g-Vb21CY1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    dW = 1/m*(np.dot(dZ,A_prev.T))\n",
        "    db = 1/m*(np.sum(dZ,axis=1,keepdims=True))\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0919CQWIDWe",
        "colab_type": "text"
      },
      "source": [
        "Далее реализуем второй этап. Для этого нам надо будет собрать вместе функции: **`linear_backward`** с результатом первого этапа и функцию **`linear_activation_backward`**, которая выводит $dz^{[l]}$ с учётом используемой в слое активации.\n",
        "\n",
        "Чтобы упростить вычисления, мы уже объявили заранее две вспомогательных функции:\n",
        "- **`sigmoid_backward`**: \n",
        "\n",
        "```python\n",
        "dZ = sigmoid_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "- **`relu_backward`**: \n",
        "\n",
        "```python\n",
        "dZ = relu_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "Обозначив за $\\sigma(.)$ функцию активации, \n",
        "`sigmoid_backward` and `relu_backward` вычисляют $$dz^{[l]} = da^{[l]} * \\sigma'(z^{[l]}) \\tag{8}$$.  \n",
        "\n",
        "**Задание**: Реализуйте второй этап back_propagation (см. формулу 8)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPKrfxuhCYRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        ### START CODE HERE ###\n",
        "        #print('relu')\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        #dA_prev, dW, db = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        #print('sigm')\n",
        "        ### START CODE HERE ###\n",
        "        dZ=sigmoid_backward(dA, activation_cache)\n",
        "        #dA_prev, dW, db = None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yudRmLMcJdsK",
        "colab_type": "text"
      },
      "source": [
        "Теперь можно описать полный шаг back_propagation через всю нейросеть. Для ускорения этого процесса мы записывали в массив caches все необходимые промежуточные результаты. Картинка ниже еще раз напоминает, что back_propagation работает в \"обратном направлении\", то есть от последнего слоя к первому:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vpyvdh9LRhO_9pwuG53tfwzcfJmVPJum\" style=\"width:665px;height:320px;\">\n",
        "\n",
        "**Задание**: реализуйте back_propagation для всей нейросети.\n",
        "\n",
        "Градиент последнего слоя уже посчитан для удобства :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emLKXmKvK_-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# GRADED FUNCTION: L_model_backward\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # кол-во слоев\n",
        "    m = AL.shape[1]\n",
        "   \n",
        "    \n",
        "    # Инициализация backpropagation\n",
        "    #print(\"Y shape is\", Y.shape)\n",
        "    #print(\"AL shape is\", AL.shape)\n",
        "    Y=Y.T\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        " \n",
        "    assert(dAL.shape == AL.shape)\n",
        "    #print(\"dAL shape is \", dAL.shape)\n",
        "    # Градиент слоя (SIGMOID -> LINEAR). Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    ### START CODE HERE ### \n",
        "    current_cache = caches[L-1] \n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\")\n",
        "    ### END CODE HERE ###\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # градиенты l-ого слоя (RELU -> LINEAR).\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        ### START CODE HERE ###\n",
        "        current_cache = caches[l] \n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache,\"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0N5W_6sLucl",
        "colab_type": "text"
      },
      "source": [
        "### Шаг 5. Обновление параметров\n",
        "\n",
        "Разумеется, после подсчета градиентов функции ошибки по соответствующим параметрам, параметры надо обновить. Обновляем по тем же формулам, что и раньше:\n",
        "\n",
        "$$ w^{[l]} = w^{[l]} - \\alpha \\text{ } dw^{[l]} \\tag{9}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}, \\tag{10}$$\n",
        "\n",
        "где $\\alpha$ - это learning_rate.\n",
        "\n",
        "**Задание**: реализуйте обновление параметров нейросети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN-Jgs--MRuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    for l in range(L):\n",
        "\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    ### END CODE HERE ###\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iktr3xtuMnQm",
        "colab_type": "text"
      },
      "source": [
        "Итак, мы реализовали все блоки для того, чтобы собрать глубокую нейросеть!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlTtWpao8GlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "layers_dims = [4096, 20, 7, 5, 1] #  4-layer model\n",
        "layers_dims = [2, 20, 7, 5, 1] #  4-layer model\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (≈ 1 line of code)\n",
        "    ### START CODE HERE ###\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        \n",
        "        cost = compute_cost(AL, Y)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "        # Backward propagation.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        ### END CODE HERE ###\n",
        " \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    print(costs)\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IUFbj8f85Wv5",
        "outputId": "4ccd97ef-f543-4cf5-d6dc-9ca0338847bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#print(X_train.shape)\n",
        "\n",
        "L_layer_model(X_train,np.array(Y_train), layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 782.885139\n",
            "Cost after iteration 100: 168.488001\n",
            "Cost after iteration 200: 153.014985\n",
            "Cost after iteration 300: 144.588965\n",
            "Cost after iteration 400: 138.880168\n",
            "Cost after iteration 500: 134.699690\n",
            "Cost after iteration 600: 131.542987\n",
            "Cost after iteration 700: 128.672489\n",
            "Cost after iteration 800: 126.121447\n",
            "Cost after iteration 900: 123.754237\n",
            "Cost after iteration 1000: 121.520673\n",
            "Cost after iteration 1100: 119.397867\n",
            "Cost after iteration 1200: 117.350081\n",
            "Cost after iteration 1300: 115.000549\n",
            "Cost after iteration 1400: 112.828292\n",
            "Cost after iteration 1500: 110.885838\n",
            "Cost after iteration 1600: 108.993522\n",
            "Cost after iteration 1700: 106.794164\n",
            "Cost after iteration 1800: 104.715203\n",
            "Cost after iteration 1900: 102.831566\n",
            "Cost after iteration 2000: 100.851798\n",
            "Cost after iteration 2100: 98.685589\n",
            "Cost after iteration 2200: 95.354046\n",
            "Cost after iteration 2300: 92.916476\n",
            "Cost after iteration 2400: 90.890059\n",
            "Cost after iteration 2500: 89.240667\n",
            "Cost after iteration 2600: 87.798896\n",
            "Cost after iteration 2700: 86.441938\n",
            "Cost after iteration 2800: 85.152817\n",
            "Cost after iteration 2900: 83.918299\n",
            "[array(782.88513889), array(168.48800114), array(153.01498453), array(144.58896459), array(138.8801679), array(134.69969042), array(131.5429868), array(128.67248928), array(126.12144729), array(123.75423666), array(121.52067274), array(119.39786742), array(117.35008055), array(115.00054862), array(112.82829237), array(110.885838), array(108.99352197), array(106.7941639), array(104.71520263), array(102.83156614), array(100.85179802), array(98.68558887), array(95.35404574), array(92.91647621), array(90.89005883), array(89.24066683), array(87.7988955), array(86.44193821), array(85.15281686), array(83.91829874)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X+cXHV97/HXe3dmdzb7I7+NIQkE\nELVWRTEKPLReKtYWag1WpHpVouXeaC+2VnsflVofVdtyL2ot1euVikUNrT9A1BK51Jai+BskQX6j\nEiIhCYEsIT92N/t7P/eP853dyWZmd/NjdnZ33s/HYx5zzvd858z37CTzme/3e87nKCIwMzMbr6HW\nDTAzs5nJAcLMzMpygDAzs7IcIMzMrCwHCDMzK8sBwszMynKAsDlH0r9JWlfrdpjNdg4QdtxIelTS\nq2vdjog4LyI21LodAJJuk/TfpuF9miV9XtIBSU9Iet8k9d+b6h1Ir2su2bZa0nclHZT089LPVNI/\nSuouefRL6irZfpukvpLtv6jOEdt0cICwWUVSrtZtKJpJbQE+DJwGnAT8JvDnkn6nXEVJvw1cBpyb\n6p8CfKSkyleAnwGLgb8EbpC0FCAi3hURbcVHqvu1cW/x7pI6zzleB2jTzwHCpoWk10q6W9I+ST+W\n9MKSbZdJekRSl6QHJb2+ZNvbJf1I0pWS9gAfTmU/lPR3kvZK+pWk80peM/qrfQp1T5b0/fTe/ynp\n/0r6lwrHcI6kHZLeL+kJ4AuSFkq6SVJn2v9Nklam+pcDvwF8Ov2a/nQqf66kWyQ9LekXki46Dn/i\ndcDfRMTeiHgI+Bzw9gnqXhMRD0TEXuBvinUlPRs4A/hQRPRGxNeB+4A3lPl7tKbyGdFbs+PPAcKq\nTtKLgc8D7yT7VfpZYGPJsMYjZF+k88l+yf6LpOUluzgT2AosAy4vKfsFsAT4GHCNJFVowkR1vwz8\nNLXrw8DbJjmcZwKLyH55ryf7P/SFtH4i0At8GiAi/hL4AWO/qN+dvlRvSe/7DOBNwGckPa/cm0n6\nTAqq5R73pjoLgeXAPSUvvQf49QrH8Otl6i6TtDht2xoRXeO2l9vXG4BO4Pvjyv+3pKdSYD+nQhts\nFnCAsOmwHvhsRNwREcNpfqAfOAsgIr4WEY9HxEhEXAc8DLys5PWPR8T/iYihiOhNZdsi4nMRMUz2\nC3Y5WQApp2xdSScCLwX+KiIGIuKHwMZJjmWE7Nd1f/qFvScivh4RB9OX6uXAf5ng9a8FHo2IL6Tj\n+RnwdeCN5SpHxP+IiAUVHsVeWFt63l/y0v1Ae4U2tJWpS6o/fttE+1oHXBuHJnR7P9mQ1QrgauBb\nkk6t0A6b4RwgbDqcBPxZ6a9fYBVwAoCki0uGn/YBzyf7tV+0vcw+nyguRMTBtNhWpt5EdU8Ani4p\nq/RepTojoq+4ImmepM9K2ibpANmv6QWSGiu8/iTgzHF/i7eQ9UyOVnd67igp6wC6ytQt1h9fl1R/\n/Lay+0rB9Rzg2tLy9COgKwXQDcCPgPOndhg20zhA2HTYDlw+7tfvvIj4iqSTyMbL3w0sjogFwP1A\n6XBRtVIO7wIWSZpXUrZqkteMb8ufAc8BzoyIDuCVqVwV6m8Hvjfub9EWEX9U7s3KnDVU+ngAIM0j\n7AJOL3np6cADFY7hgTJ1n4yIPWnbKZLax20fv6+3AT+KiK0V3qMoOPSztFnEAcKOt7ykQskjRxYA\n3iXpTGVaJf1u+hJqJfsS6QSQ9A6yHkTVRcQ2YBPZxHeTpLOB3zvC3bSTzTvsk7QI+NC47U+SDbkU\n3QQ8W9LbJOXT46WSfq1CGw85a2jco3Re4Frgg2nS/LnAfwe+WKHN1wKXSHqepAXAB4t1I+KXwN3A\nh9Ln93rghWTDYKUuHr9/SQsk/Xbxc5f0FrKA+e0K7bAZzgHCjrebyb4wi48PR8Qmsi+sTwN7gS2k\ns2Yi4kHgE8BPyL5MX0A2LDFd3gKcDewB/ha4jmx+ZKr+AWgBngJu5/Avw08CF6YznD6V5ileQzY5\n/TjZ8NdHgWaOzYfIJvu3Ad8DPh4R34ZsOCj1OE4ESOUfA74LPJZeUxrY3gSsIfusrgAujIjO4sYU\nSFdy+OmtebK/YSfZ3+OPgQtS0LFZSL5hkNkYSdcBP4+I8T0Bs7rjHoTVtTS8c6qkBmUXlq0F/rXW\n7TKbCWbSlaBmtfBM4Btk10HsAP4onXpqVvc8xGRmZmV5iMnMzMqa1UNMS5YsidWrV9e6GWZms8rm\nzZufioilk9Wb1QFi9erVbNq0qdbNMDObVSRtm0q9qg4xKcs5/4Ck+yV9JV1Ac7KkOyRtkXSdpKZU\ntzmtb0nbV1ezbWZmNrGqBQhJK4A/AdZExPOBRrILcD4KXBkRzyK7EOeS9JJLgL2p/MpUz8zMaqTa\nk9Q5oCWlW5hHli/mVcANafsG4IK0vJaxvPI3AOdOkL7ZzMyqrGoBIiJ2An9Hdin/LrKUwZuBfREx\nlKrtIEsLTHrenl47lOovHr9fSeslbZK0qbOzc/xmMzM7Tqo5xLSQrFdwMlla5Vag7C0Qj0REXB0R\nayJizdKlk07Cm5nZUarmENOrgV9FRGdEDJJdrfpyslz5xbOnVgI70/JOUqrltH0+WQI1MzOrgWoG\niMeAs9INVUR2g/QHyTJIXpjqrANuTMsb0zpp+3fCl3mbmdVMNecg7iCbbL6L7KbnDWS3IHw/8D5J\nW8jmGK5JL7kGWJzK3wdcVq223fno03zs2z9nZMTxx8yskqpeKJdSJo9Pm7yVQ+83XKzbR4X78h5v\n92zfx2due4R3nXMqHYX8dLylmdmsU5e5mIpBoatvaJKaZmb1qy4DRHsh6zgd6B2scUvMzGauOg0Q\n7kGYmU2mLgNER4t7EGZmk6nLADHag+h3gDAzq6QuA0TH6ByEh5jMzCqpywAxNgfhHoSZWSV1GSCa\ncg0U8g0c8CS1mVlFdRkgIOtFuAdhZlZZ3QaIjkLOcxBmZhOo2wDRXshzwD0IM7OK6jZAdLTkPQdh\nZjaBug0Q7YWc5yDMzCZQtwGio5B3qg0zswnUcYDIOdWGmdkE6jZAtBdy9A+N0D80XOummJnNSHUb\nIDpanNHVzGwidRsgiveEcIAwMyuvbgNE8a5ynocwMyuvbgOEbxpkZjaxqgUISc+RdHfJ44CkP5W0\nSNItkh5OzwtTfUn6lKQtku6VdEa12gYlNw3ytRBmZmVVLUBExC8i4kUR8SLgJcBB4JvAZcCtEXEa\ncGtaBzgPOC091gNXVatt4JTfZmaTma4hpnOBRyJiG7AW2JDKNwAXpOW1wLWRuR1YIGl5tRrkmwaZ\nmU1sugLEm4CvpOVlEbErLT8BLEvLK4DtJa/ZkcoOIWm9pE2SNnV2dh51g1qbckjuQZiZVVL1ACGp\nCXgd8LXx2yIigDiS/UXE1RGxJiLWLF269Kjb1dAg2ptzTthnZlbBdPQgzgPuiogn0/qTxaGj9Lw7\nle8EVpW8bmUqqxqn/DYzq2w6AsSbGRteAtgIrEvL64AbS8ovTmcznQXsLxmKqoqOFifsMzOrJFfN\nnUtqBX4LeGdJ8RXA9ZIuAbYBF6Xym4HzgS1kZzy9o5ptg+xqal8oZ2ZWXlUDRET0AIvHle0hO6tp\nfN0ALq1me8brKOR4fF/fdL6lmdmsUbdXUkOWbsNzEGZm5dV1gMjuKuc5CDOzcuo6QGST1INko1tm\nZlaqrgNEeyHHSEDPgG8aZGY2Xl0HCKf8NjOrrK4DhFN+m5lVVtcBwim/zcwqq+sA4ZTfZmaV1XWA\ncMpvM7PK6jpAuAdhZlZZnQeI4hyEexBmZuPVdYAo5BtpyjV4ktrMrIy6DhCQzUP4NFczs8M5QBTy\nvlDOzKyMug8QTthnZlZe3QeIjhan/DYzK6fuA4R7EGZm5dV9gPAchJlZeXUfINyDMDMrr6oBQtIC\nSTdI+rmkhySdLWmRpFskPZyeF6a6kvQpSVsk3SvpjGq2raijkKd3cJjB4ZHpeDszs1mj2j2ITwLf\njojnAqcDDwGXAbdGxGnArWkd4DzgtPRYD1xV5bYBY1dTuxdhZnaoqgUISfOBVwLXAETEQETsA9YC\nG1K1DcAFaXktcG1kbgcWSFperfYVdbT4pkFmZuVUswdxMtAJfEHSzyT9k6RWYFlE7Ep1ngCWpeUV\nwPaS1+9IZYeQtF7SJkmbOjs7j7mRvmmQmVl51QwQOeAM4KqIeDHQw9hwEgAREUAcyU4j4uqIWBMR\na5YuXXrMjRxN+e1rIczMDlHNALED2BERd6T1G8gCxpPFoaP0vDtt3wmsKnn9ylRWVU75bWZWXtUC\nREQ8AWyX9JxUdC7wILARWJfK1gE3puWNwMXpbKazgP0lQ1FV45TfZmbl5aq8/z8GviSpCdgKvIMs\nKF0v6RJgG3BRqnszcD6wBTiY6ladJ6nNzMqraoCIiLuBNWU2nVumbgCXVrM95bQ1+zRXM7Ny6v5K\n6sYG0d6c8yS1mdk4dR8gwOk2zMzKcYAgpfz2HISZ2SEcIHAPwsysHAcIUspvz0GYmR3CAQL3IMzM\nynGAwLcdNTMrxwGCsR5EdimGmZmBAwSQzUEMjwQHB4Zr3RQzsxnDAQKn/DYzK8cBgtK7ynkewsys\nyAGCkoR9DhBmZqMcIHDKbzOzchwgyCapwSm/zcxKOUAwdttRT1KbmY1xgMBzEGZm5ThAAM25BvKN\ncg/CzKyEAwQgKUvY5zkIM7NRDhCJE/aZmR2qqgFC0qOS7pN0t6RNqWyRpFskPZyeF6ZySfqUpC2S\n7pV0RjXbNp4T9pmZHWo6ehC/GREviog1af0y4NaIOA24Na0DnAeclh7rgaumoW2j3IMwMztULYaY\n1gIb0vIG4IKS8msjczuwQNLy6WqU5yDMzA5V7QARwH9I2ixpfSpbFhG70vITwLK0vALYXvLaHans\nEJLWS9okaVNnZ+dxa6h7EGZmh8pVef+viIidkp4B3CLp56UbIyIkHdFNGCLiauBqgDVr1hy3Gzi0\nF/JO1mdmVqKqPYiI2JmedwPfBF4GPFkcOkrPu1P1ncCqkpevTGXToqOQp2dgmKHhkel6SzOzGa1q\nAUJSq6T24jLwGuB+YCOwLlVbB9yYljcCF6ezmc4C9pcMRVVdMWFfd7+HmczMoLpDTMuAb0oqvs+X\nI+Lbku4Erpd0CbANuCjVvxk4H9gCHATeUcW2HWY03UbvEAvmNU3nW5uZzUhVCxARsRU4vUz5HuDc\nMuUBXFqt9kxmLOW35yHMzMBXUo8aTfntAGFmBjhAjGp3ym8zs0M4QCTzW3zTIDOzUg4QiXsQZmaH\ncoBI2po9SW1mVmpKAULSG6dSNpvlGhtobWp0D8LMLJlqD+Ivplg2q7U7YZ+Z2agJr4OQdB7ZxWsr\nJH2qZFMHMOd+ane0OGGfmVnRZBfKPQ5sAl4HbC4p7wLeW61G1Up7IU9Xv3sQZmYwSYCIiHuAeyR9\nOSIGAdId4FZFxN7paOB06ijkeKp7oNbNMDObEaY6B3GLpA5Ji4C7gM9JurKK7aoJp/w2Mxsz1QAx\nPyIOAL9Pdte3MymTT2m262jJccBzEGZmwNQDRC7du+Ei4KYqtqemij2ILG+gmVl9m2qA+Gvg34FH\nIuJOSacAD1evWbXRUcgzOBz0DfqmQWZmU0r3HRFfA75Wsr4VeEO1GlUrY+k2Bmlpaqxxa8zMamuq\nV1KvlPRNSbvT4+uSVla7cdNt9KZBnqg2M5vyENMXyG4JekJ6fCuVzSljNw3yRLWZ2VQDxNKI+EJE\nDKXHF4GlVWxXTYzeNMjpNszMphwg9kh6q6TG9HgrsKeaDauFDqf8NjMbNdUA8Ydkp7g+AewCLgTe\nPpUXpoDyM0k3pfWTJd0haYuk6yQ1pfLmtL4lbV99hMdyzNp921Ezs1FHcprruohYGhHPIAsYH5ni\na98DPFSy/lHgyoh4FrAXuCSVXwLsTeVXpnrTqqPFPQgzs6KpBogXluZeioingRdP9qJ0ptPvAv+U\n1gW8CrghVdkAXJCW16Z10vZzU/1p05JvpLFBTrdhZsbUA0RDStIHQMrJNJVrKP4B+HOgeOXZYmBf\nRBR/ou8AVqTlFcB2gLR9f6o/bSTRUchxoNc9CDOzKV0oB3wC+Imk4sVybwQun+gFkl4L7I6IzZLO\nOfomHrbf9cB6gBNPPPF47XaUE/aZmWWmeiX1tZI2kQ0PAfx+RDw4ycteDrxO0vlAgewmQ58EFkjK\npV7CSmBnqr8TWAXskJQD5lPmTKmIuBq4GmDNmjXHPWmSE/aZmWWmOsRERDwYEZ9Oj8mCAxHxFxGx\nMiJWA28CvhMRbwG+S3YWFMA64Ma0vDGtk7Z/J2qQNa+92T0IMzM4ggBxHL0feJ+kLWRzDNek8muA\nxan8fcBlNWhb1oPwHISZ2ZTnII5JRNwG3JaWtwIvK1Onj2xuo6Y8B2FmlqlFD2JG6yjkPQdhZoYD\nxGHaCzm6+4cYHvFNg8ysvjlAjFNM+d3tXoSZ1TkHiHHGUn57HsLM6psDxDgdDhBmZoADxGGK94Rw\nwj4zq3cOEOO0O0CYmQEOEIcppvz2XeXMrN45QIwz1oNwgDCz+uYAMc7YWUweYjKz+uYAMU6+sYGW\nfKN7EGZW9xwgynDCPjMzB4iy2gt5uvrdgzCz+uYAUYZvO2pm5gBRllN+m5k5QJTV0eKU32ZmDhBl\ntBdy7kGYWd1zgCij3XMQZmYOEOV0FPIMDI/QNzhc66aYmdWMA0QZxZTfTthnZvWsagFCUkHSTyXd\nI+kBSR9J5SdLukPSFknXSWpK5c1pfUvavrpabZtM8a5yvieEmdWzavYg+oFXRcTpwIuA35F0FvBR\n4MqIeBawF7gk1b8E2JvKr0z1aqLdPQgzs+oFiMh0p9V8egTwKuCGVL4BuCAtr03rpO3nSlK12jeR\n4k2DnPLbzOpZVecgJDVKuhvYDdwCPALsi4jiT/MdwIq0vALYDpC27wcWl9nnekmbJG3q7OysSrt9\n0yAzsyoHiIgYjogXASuBlwHPPQ77vDoi1kTEmqVLlx5zG8sZvWmQ5yDMrI5Ny1lMEbEP+C5wNrBA\nUi5tWgnsTMs7gVUAaft8YM90tG883zTIzKy6ZzEtlbQgLbcAvwU8RBYoLkzV1gE3puWNaZ20/TsR\nEdVq30RamxppEL5YzszqWm7yKkdtObBBUiNZILo+Im6S9CDwVUl/C/wMuCbVvwb4Z0lbgKeBN1Wx\nbROS5IR9Zlb3qhYgIuJe4MVlyreSzUeML+8D3lit9hypjpacE/aZWV3zldQVtDe7B2Fm9c0BogIn\n7DOzeucAUUF2Twj3IMysfjlAVJDdE8I9CDOrXw4QFXQU3IMws/rmAFFBRyFHd/8QIyM1uRTDzKzm\nHCAq6GjJEwHdAx5mMrP65ABRgVN+m1m9c4CowCm/zazeOUBU4JTfZlbvHCAqGE357R6EmdUpB4gK\nRnsQ/Q4QZlafHCAq6CgUexAeYjKz+uQAUYFvGmRm9c4BooKmXAPNuQan/DazuuUAMYGOFqf8NrP6\n5QAxgfaCbxpkZvXLAWICHYW8T3M1s7rlADEBp/w2s3pWtQAhaZWk70p6UNIDkt6TyhdJukXSw+l5\nYSqXpE9J2iLpXklnVKttU+WbBplZPatmD2II+LOIeB5wFnCppOcBlwG3RsRpwK1pHeA84LT0WA9c\nVcW2TUmHexBmVseqFiAiYldE3JWWu4CHgBXAWmBDqrYBuCAtrwWujcztwAJJy6vVvqnwHISZ1bNp\nmYOQtBp4MXAHsCwidqVNTwDL0vIKYHvJy3aksvH7Wi9pk6RNnZ2dVWszZHMQ/UMj9A8NV/V9zMxm\noqoHCEltwNeBP42IA6XbIiKAI7plW0RcHRFrImLN0qVLj2NLD9fR4oyuZla/qhogJOXJgsOXIuIb\nqfjJ4tBRet6dyncCq0pevjKV1YxvGmRm9ayaZzEJuAZ4KCL+vmTTRmBdWl4H3FhSfnE6m+ksYH/J\nUFRN+KZBZlbPclXc98uBtwH3Sbo7lX0AuAK4XtIlwDbgorTtZuB8YAtwEHhHFds2Jb5pkJnVs6oF\niIj4IaAKm88tUz+AS6vVnqNRHGLytRBmVo98JfUExiapHSDMrP44QEzAk9RmVs8cICbQ1pRDgh8/\nsoed+3pr3Rwzs2nlADGBhgbxxpes5LZf7OY3Pvod1l+7iR8+/BTZdImZ2dym2fxlt2bNmti0aVPV\n32fH3oN86Y7HuO7O7TzdM8ApS1p561kn8YaXrGR+mqcwM5stJG2OiDWT1nOAmLr+oWFuvm8X1/5k\nGz97bB8t+UYuePEKLj77JH5tece0tcPM7Fg4QFTZ/Tv3888/2caN9+ykb3CENSct5E0vO5GzT13M\nCfMLZNcJmpnNPA4Q02T/wUG+tnk7/3L7Nh7dcxCAZ3YUeMlJCznjpIW85KSFPG95B005T/eY2czg\nADHNRkaCB3cd4K7H9rJ5W/bYsTc786k518DpKxeMBowzTlzA4rbmGrfYzOqVA8QM8OSBvtFgsXnb\nXh54fD+Dw9nfe1lHM6cubeOUpa2csqSNU5/RxilLWlmxoIWGBg9PmVn1TDVAVDMXU91b1lHg/Bcs\n5/wXZPc96hsc5r6d+9m8bS+/fLKLrZ093Hj344dciFfIN7B6cSunLm3j1KWtrF7SygkLWlg+v8Az\n5xdozjXW6nDMrM44QEyjQr6Rl65exEtXLxotiwie6h5ga2c3j3T2sLWzm61P9XD/4/v5t/t3MTKu\ng7ekrYnl87OAccKCFp45vzC6/Iz2Zha1NtHWnPMkuZkdMweIGpPE0vZmlrY3c+Ypiw/Z1j80zI69\nveza18fj+7PnJw708vi+Ph7d08NPHtlDV//haUCacg0saW1icVsWMBa3NbGkrZnFrU0sas2WF7Y2\nsWheEwta87Q7oJhZGQ4QM1hzrjENNbVVrNPVN8gT+/t4fH8fT3X1s6ennz3dAzzVPcDTPf3s6Rlg\ny+5unurup39opOw+cg0aDRgLW/MsnNc0FkDm5ZnfkmfBvKb0nGdBS56OljyFvIe7zOYyB4hZrr2Q\np72Q57Rl7RPWiwh6BoZ5unuAp3r62XdwgKd7BtnbM8DTBwfS+gB7ewZ5eHc3e3sG2Htw4LAhrlKF\nfEMWNFqy4NHRkqejJUdHIT+2XsiVLOeZPy9PeyFHW1POk/FmM5wDRJ2QRFtzjrbmHCcunjel14yM\nBF39QxzoHWTfwUH29Q6wPy3v7x1My2NlO/f18tCuQQ70DU6aAVeCtuYsmLQXsueOlhzthSyotKfy\n9kKetkIu1cnR1pwCjIOMWdU5QFhFDQ1ifkvWG1i1aPL6pYZHgu6+IQ70ZYHkQO/g6HJX31BaH8qW\n+wbp6hvk8X19dPV3caB3iK6+wQl7L5CCTFNuNIC0NedoK2RzKm3NubFAUlxOwaW0Z9NeyPsiRrMK\nHCCsKhobxPx52ZDSqqN4fURwcGCYrr4sWHT1D40ud/cNHVbe3TdEd/8Q+3sH2bn3IN39WVnPwPCk\n79WSbyw7NFbai2kf7cUcXtbqnozNUQ4QNiNJorU5R2tzjmfOLxz1foZHIgsWKWB09WU9mQO9Q+m5\n2MNJ632DdHb1s2V3dxaA+oYYmqQrI0FrU47W5kZam3O0p3YftlzI0dqU1WkrKc+WG0fL8o3u0djM\nULUAIenzwGuB3RHx/FS2CLgOWA08ClwUEXuVnWP5SeB84CDw9oi4q1pts/rRWDJMdjQigr7BkRRY\nhkaDRlfpcv8QPSkAdQ+knkv/EE/3ZD2ZnhSgilfRT6Yp10B7CijtxbmZ5kN7Lh0t+dF5mfZC/pAh\ntfbmPIV8g09dtmNWzR7EF4FPA9eWlF0G3BoRV0i6LK2/HzgPOC09zgSuSs9mNSWJlqZGWpoaecYx\nZnTvGxymp3+Inv7hLHAMDI0GkCyIDI8ulw6pdfUN0dnVPTo3M5Vhs1yDRudfiicDFHs4bSU9l7Ge\nTGPZ8vZCjuacg029qlqAiIjvS1o9rngtcE5a3gDcRhYg1gLXRpYY6nZJCyQtj4hd1Wqf2XQr5Bsp\n5BtZXPmylikpPQFgdF4m9VIO9I0NpRWH1Q70DdHdP0hndz+P7hnr1RycQqCBLNiUBo7SwNNW0tMp\nzskUz0RrG1fuNDGzz3TPQSwr+dJ/AliWllcA20vq7UhlhwUISeuB9QAnnnhi9VpqNkOVngBwLIZH\ngoMDJT2akuGwnoHixP8w3f2Dhyz39A+zr3eQHelkgK6+qQWbtuYcS9JV/UvamlnSXrLc1szStO50\nMTNHzSapIyIkHXEq2Yi4Grgasmyux71hZnWisUGjF1oeq6HhkdFgcaDMPM2B3kH29AzwVHc/T3X3\ns6Wzm9t/1c++g4MV21Y8FXnsIsz82Ho642xs3qXklObmbDgt58n+YzbdAeLJ4tCRpOXA7lS+Ew45\nG3JlKjOzWSDX2MCCeU0smNd0RK8bHB5JqWH60yNLEVO8EHN/79DodTQ79/aOlk92Zhlkpy8Xg0dr\nc455TY3pMbbc0pSdWdaSylubs2HAlnxW1pKGBYvL85oa62pOZroDxEZgHXBFer6xpPzdkr5KNjm9\n3/MPZnNfvrGBZ6ZU9lNVvEZmf+/gaK+lON/S3Z/1WHqKQ2Ml2w8ODNPZ3c/BgYP0DmQnBPQODk/5\n7LJSWeBoGA0mzWm9GFCK2wr5Rgq50vWG0bLmfAPNuXF1S8tK6jTW6Dqbap7m+hWyCeklknYAHyIL\nDNdLugTYBlyUqt9MdorrFrLTXN9RrXaZ2exWeo3M8TAwNELvwDAHB7Mg0jswTN/gML2D2XLvYFof\nGKZ3cOSQ9WK9vsER+oeysgN9g2nbWFnf0AjDU+j1VJJv1CEBoznfwHtf/Wx+7/QTjsvfoJJqnsX0\n5gqbzi1TN4BLq9UWM7NKmnINNOUamM+xz8VMZGh4hL6hEfpSgOkbzJb7h4bpHxyhb2gsqPQNjtA/\nODxav7/M84JjPElhKnwltZnZNMg1NtDW2EDbcer5TAdP85uZWVkOEGZmVpYDhJmZleUAYWZmZTlA\nmJlZWQ4QZmZWlgOEmZmV5QBhZmZlKbuIeXaS1EmWsuNoLAGeOo7NmQnm2jHNteOBuXdMc+14YO4d\nU7njOSkilk72wlkdII6FpE2ZfEv7AAAHgklEQVQRsabW7Tie5toxzbXjgbl3THPteGDuHdOxHI+H\nmMzMrCwHCDMzK6ueA8TVtW5AFcy1Y5prxwNz75jm2vHA3Dumoz6eup2DMDOzidVzD8LMzCbgAGFm\nZmXVZYCQ9DuSfiFpi6TLat2eYyXpUUn3Sbpb0qZat+doSPq8pN2S7i8pWyTpFkkPp+eFtWzjkahw\nPB+WtDN9TndLOr+WbTxSklZJ+q6kByU9IOk9qXxWfk4THM+s/ZwkFST9VNI96Zg+kspPlnRH+s67\nTlLTlPZXb3MQkhqBXwK/BewA7gTeHBEP1rRhx0DSo8CaiJi1F/dIeiXQDVwbEc9PZR8Dno6IK1Ig\nXxgR769lO6eqwvF8GOiOiL+rZduOlqTlwPKIuEtSO7AZuAB4O7Pwc5rgeC5iln5OkgS0RkS3pDzw\nQ+A9wPuAb0TEVyX9I3BPRFw12f7qsQfxMmBLRGyNiAHgq8DaGrep7kXE94GnxxWvBTak5Q1k/3ln\nhQrHM6tFxK6IuCstdwEPASuYpZ/TBMcza0WmO63m0yOAVwE3pPIpf0b1GCBWANtL1ncwy/9RkP0D\n+A9JmyWtr3VjjqNlEbErLT8BLKtlY46Td0u6Nw1BzYqhmHIkrQZeDNzBHPicxh0PzOLPSVKjpLuB\n3cAtwCPAvogYSlWm/J1XjwFiLnpFRJwBnAdcmoY35pTIxkJn+3joVcCpwIuAXcAnatucoyOpDfg6\n8KcRcaB022z8nMocz6z+nCJiOCJeBKwkGzF57tHuqx4DxE5gVcn6ylQ2a0XEzvS8G/gm2T+KueDJ\nNE5cHC/eXeP2HJOIeDL95x0BPscs/JzSuPbXgS9FxDdS8az9nModz1z4nAAiYh/wXeBsYIGkXNo0\n5e+8egwQdwKnpVn9JuBNwMYat+moSWpNE2xIagVeA9w/8atmjY3AurS8Drixhm05ZsUv0eT1zLLP\nKU2AXgM8FBF/X7JpVn5OlY5nNn9OkpZKWpCWW8hOxnmILFBcmKpN+TOqu7OYANJpa/8ANAKfj4jL\na9ykoybpFLJeA0AO+PJsPB5JXwHOIUtN/CTwIeBfgeuBE8nSul8UEbNi4rfC8ZxDNmwRwKPAO0vG\n7mc8Sa8AfgDcB4yk4g+QjdvPus9pguN5M7P0c5L0QrJJ6EayDsD1EfHX6Xviq8Ai4GfAWyOif9L9\n1WOAMDOzydXjEJOZmU2BA4SZmZXlAGFmZmU5QJiZWVkOEGZmVpYDhM1Ikn6cnldL+q/Hed8fKPde\n1SLpAkl/VaV9f2DyWke8zxdI+uLx3q/NPj7N1WY0SecA/zMiXnsEr8mV5J0pt707ItqOR/um2J4f\nA6871my75Y6rWsci6T+BP4yIx473vm32cA/CZiRJxYyUVwC/kfLyvzclIvu4pDtTMrV3pvrnSPqB\npI3Ag6nsX1MCwweKSQwlXQG0pP19qfS9lPm4pPuV3V/jD0r2fZukGyT9XNKX0lW4SLpC2f0E7pV0\nWHpoSc8G+ovBQdIXJf2jpE2Sfinptal8ysdVsu9yx/JWZfcDuFvSZ5Wlt0dSt6TLld0n4HZJy1L5\nG9Px3iPp+yW7/xZZlgGrZxHhhx8z7kGWjx+yq49vKilfD3wwLTcDm4CTU70e4OSSuovScwtZuoTF\npfsu815vIMt+2UiWkfQxYHna936yHDYNwE+AVwCLgV8w1hNfUOY43gF8omT9i8C3035OI8usWTiS\n4yrX9rT8a2Rf7Pm0/hng4rQcwO+l5Y+VvNd9wIrx7QdeDnyr1v8O/Kjto5i8yWy2eA3wQknFvDLz\nyb5oB4CfRsSvSur+iaTXp+VVqd6eCfb9CuArETFMloDue8BLgQNp3zsAUirl1cDtQB9wjaSbgJvK\n7HM50Dmu7PrIEsE9LGkrWbbNIzmuSs4FXgLcmTo4LYwlzhsoad9mshw9AD8CvijpeuAbY7tiN3DC\nFN7T5jAHCJttBPxxRPz7IYXZXEXPuPVXA2dHxEFJt5H9Uj9apXlrhoFcRAxJehnZF/OFwLvJbsxS\nqpfsy77U+Im/YIrHNQkBGyLiL8psG4yI4vsOk/7vR8S7JJ0J/C6wWdJLImIP2d+qd4rva3OU5yBs\npusC2kvW/x34o5SmGUnPTllsx5sP7E3B4bnAWSXbBouvH+cHwB+k+YClwCuBn1ZqmLL7CMyPiJuB\n9wKnl6n2EPCscWVvlNQg6VTgFLJhqqke13ilx3IrcKGkZ6R9LJJ00kQvlnRqRNwREX9F1tMppsJ/\nNrMoi6lVh3sQNtPdCwxLuods/P6TZMM7d6WJ4k7K3z7x28C7JD1E9gV8e8m2q4F7Jd0VEW8pKf8m\nWe78e8h+1f95RDyRAkw57cCNkgpkv97fV6bO94FPSFLJL/jHyAJPB/CuiOiT9E9TPK7xDjkWSR8k\nu7tgAzAIXEqWYbWSj0s6LbX/1nTsAL8J/L8pvL/NYT7N1azKJH2SbML3P9P1BTdFxA2TvKxmJDUD\n3yO7U2HF04Vt7vMQk1n1/S9gXq0bcQROBC5zcDD3IMzMrCz3IMzMrCwHCDMzK8sBwszMynKAMDOz\nshwgzMysrP8P6W1vvTP399kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[ 1.80079583,  0.35319705],\n",
              "        [-0.4655424 , -1.68817964],\n",
              "        [-0.43058423, -0.30065297],\n",
              "        [-0.01458692, -0.59346876],\n",
              "        [ 0.43431137, -0.98229572],\n",
              "        [-0.79963207,  1.30381976],\n",
              "        [ 0.92299116,  1.6912455 ],\n",
              "        [-0.01846461, -0.44198874],\n",
              "        [-0.54341161, -1.57818906],\n",
              "        [ 1.02236778, -1.01675511],\n",
              "        [-1.36800723,  0.01632185],\n",
              "        [ 1.45897339,  0.205111  ],\n",
              "        [-0.77954772, -0.83410937],\n",
              "        [ 0.64319775, -0.21631725],\n",
              "        [-0.86808622,  0.31073377],\n",
              "        [ 0.82859294,  1.90464769],\n",
              "        [-1.21395716, -0.81695794],\n",
              "        [-0.77730675, -2.49935834],\n",
              "        [-0.93979574, -1.13305514],\n",
              "        [ 1.10088088, -0.00612504]]),\n",
              " 'W2': array([[-1.62328545e+00,  5.85296683e-01, -3.70598412e-01,\n",
              "         -1.75996328e+00, -5.96670757e-01, -4.05383503e-01,\n",
              "         -8.42019177e-01,  2.46510987e-02, -2.31161737e+00,\n",
              "         -2.69131234e-01,  1.06878350e+00,  8.52797841e-01,\n",
              "          1.07774681e+00,  1.11939066e+00,  1.53439120e+00,\n",
              "         -1.04700040e+00,  8.42626120e-01, -1.95821939e+00,\n",
              "         -6.65883508e-01, -1.91452289e+00],\n",
              "        [ 1.02824027e+00,  1.24600706e+00, -2.56062333e-01,\n",
              "          1.71326379e+00, -8.41980538e-01, -9.50319027e-02,\n",
              "          1.69432162e-01, -1.11789836e+00,  4.08279929e-01,\n",
              "          1.85571278e+00, -1.10388183e+00,  8.94620934e-01,\n",
              "         -2.63678316e-01,  1.82756969e+00,  4.53315275e-01,\n",
              "          6.90577321e-01,  4.34812910e-01,  1.99657633e-01,\n",
              "         -1.49553174e+00,  1.42563259e+00],\n",
              "        [-1.36795440e+00,  2.38563192e-01,  6.22060065e-01,\n",
              "         -8.38262865e-01,  1.44760803e-01,  1.00791964e+00,\n",
              "         -8.32227147e-02, -8.88657418e-01, -2.91574461e+00,\n",
              "         -9.72101887e-01, -6.66918239e-01, -5.18003428e-01,\n",
              "         -9.41387046e-01,  3.77295221e-01, -6.30431801e-01,\n",
              "         -2.01485465e-01,  6.85508955e-01, -8.55439982e-01,\n",
              "         -2.93442451e-01,  2.15633292e+00],\n",
              "        [ 9.14835958e-01, -1.23670555e+00, -3.65148856e-02,\n",
              "          6.49186930e-01,  1.50934176e+00,  3.93480722e-01,\n",
              "         -5.80254916e-01, -6.55332813e-01,  6.08719939e-01,\n",
              "          4.93779542e-01, -5.71717418e-01, -1.71946825e-01,\n",
              "         -3.53431925e-01,  6.32289826e-01, -7.73539062e-01,\n",
              "          4.23599252e-01,  1.03251095e+00,  4.32114290e-01,\n",
              "         -7.98922510e-01,  1.30361336e+00],\n",
              "        [-6.78212679e-01,  4.60140358e-01, -7.64670275e-01,\n",
              "         -1.72493224e+00,  2.44130217e-01,  1.30406043e-01,\n",
              "         -8.36973291e-02,  2.87570571e-01, -6.26091785e-01,\n",
              "         -7.64483989e-01, -2.58645141e-01,  1.90927681e+00,\n",
              "          7.83858486e-01, -5.19991754e-01,  5.01644873e-01,\n",
              "         -4.91199024e-01, -5.23513016e-01,  8.91030242e-01,\n",
              "         -5.45151815e-01, -1.67563463e+00],\n",
              "        [-9.15150722e-01,  1.01099318e-01,  1.48912234e-01,\n",
              "          1.28871554e+00, -5.81907423e-01,  9.45038284e-01,\n",
              "          4.25129806e-01, -1.10833166e-01,  8.07910655e-01,\n",
              "          5.63055231e-01, -8.39315384e-01,  1.05546635e+00,\n",
              "         -1.17989523e+00,  1.03544100e-03,  4.16767007e-03,\n",
              "          4.32266831e-01, -6.19043625e-01, -8.42154301e-02,\n",
              "         -1.11629796e-01,  9.50004189e-01],\n",
              "        [-7.57292540e-01,  8.90445681e-02, -2.79342616e-01,\n",
              "         -1.28680085e+00,  2.69708728e+00, -7.11963834e-02,\n",
              "         -1.48750213e+00,  1.41610774e+00, -1.05823689e+00,\n",
              "          4.27471705e-01,  8.55055526e-01, -6.11699830e-01,\n",
              "         -4.60089835e-01, -5.15698623e-01, -1.34109205e-01,\n",
              "         -1.62251878e+00, -1.29897509e+00, -1.21880559e-01,\n",
              "          1.01251630e+00,  6.72751767e-01]]),\n",
              " 'W3': array([[ 1.83128474,  0.71411727, -0.46431467, -0.2235016 , -2.03775111,\n",
              "         -0.98826148, -0.92598724],\n",
              "        [ 0.47231381,  0.64556123, -0.48160296, -1.0203689 , -1.56613158,\n",
              "          0.05672054,  0.58871824],\n",
              "        [ 0.49741557, -0.81595641,  1.96577032,  1.19132158, -0.09505228,\n",
              "         -0.5984399 , -0.32161113],\n",
              "        [ 0.25964368, -0.10025621,  0.5056334 ,  0.6503458 ,  1.3438968 ,\n",
              "          1.22043525, -0.01620258],\n",
              "        [ 0.27439981, -0.41896258, -0.60259137,  2.03339041, -0.45725758,\n",
              "          1.90301663, -0.45589403]]),\n",
              " 'W4': array([[ 0.58667543,  1.50799475,  0.28998309,  0.43233546, -0.57966835]]),\n",
              " 'b1': array([[-0.03778708],\n",
              "        [-0.0723048 ],\n",
              "        [-0.04066321],\n",
              "        [ 0.13935843],\n",
              "        [-0.18467236],\n",
              "        [ 0.22581966],\n",
              "        [-0.0414647 ],\n",
              "        [-0.03102578],\n",
              "        [ 0.14380279],\n",
              "        [ 0.29983179],\n",
              "        [-0.01553613],\n",
              "        [-0.00587154],\n",
              "        [ 0.16316065],\n",
              "        [-0.0990011 ],\n",
              "        [ 0.19301527],\n",
              "        [-0.03619041],\n",
              "        [-0.03656057],\n",
              "        [ 0.05640567],\n",
              "        [-0.03308487],\n",
              "        [ 0.0953626 ]]),\n",
              " 'b2': array([[ 0.03535184],\n",
              "        [-0.0018858 ],\n",
              "        [-0.080328  ],\n",
              "        [-0.01420296],\n",
              "        [ 0.04157917],\n",
              "        [ 0.10942121],\n",
              "        [ 0.01116585]]),\n",
              " 'b3': array([[-2.69341162e-05],\n",
              "        [-8.12998472e-05],\n",
              "        [-1.45173166e-02],\n",
              "        [-1.51035110e-01],\n",
              "        [ 2.84518606e-01]]),\n",
              " 'b4': array([[-0.85436184]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG4lWUFS5WRm",
        "outputId": "5b25345a-3653-4938-d7f7-ab1d58c2e19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array([Y_train]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 280, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ24VWgWMxxD",
        "colab_type": "text"
      },
      "source": [
        "## Часть 3. Параметры и гиперпараметры модели\n",
        "\n",
        "Мы уже поговорили о том, как происходит обучение модели: для этого используются различные методы минимизации функции ошибки с помощью обновления параметров $w$ и $b$. Однако модель описывается не только этими параметрами.\n",
        "\n",
        "Learning rate $\\alpha$, кол-во слоёв в нейросети, кол-во нейронов в каждом слое, кол-во итераций обучения необходимы для описания модели. Такие параметры называются **гиперпараметрами**, они не настраиваются с помощью градиентного спуска во время обучения (это важно), а задаются экспертом. Тем не менее, в итоге гиперпараметры влияют на конечную модель, то есть на конечные значения $w$ и $b$, поэтому очень важно их правильно подбирать. Подробнее о выборе гиперпараметров мы поговорим в следующий раз."
      ]
    }
  ]
}